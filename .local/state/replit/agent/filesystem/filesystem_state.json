{"file_contents":{"README.md":{"content":"# LANCE! RSS/Atom Feed Generator\n\nUm gerador de feeds RSS e Atom n√£o oficiais para o portal de not√≠cias LANCE! Este projeto √© puramente educativo e demonstra t√©cnicas de web scraping respons√°vel e gera√ß√£o de feeds.\n\n## ‚ö†Ô∏è Aviso Importante\n\nEste projeto √© apenas para fins educativos. Respeita robots.txt, implementa delays entre requisi√ß√µes e n√£o deve ser usado para fins comerciais. Todo o conte√∫do pertence ao portal LANCE!\n\n## üöÄ Funcionalidades\n\n- **Web Scraping Respons√°vel**: Varre as p√°ginas de listagem do LANCE! respeitando robots.txt\n- **Extra√ß√£o de Metadados**: Usa JSON-LD para extrair metadados precisos dos artigos\n- **Feeds RSS e Atom**: Gera feeds padronizados com enclosures de imagem\n- **Banco de Dados SQLite**: Armazena artigos com deduplica√ß√£o\n- **Agendador Autom√°tico**: Atualiza feeds a cada 15 minutos\n- **Filtragem**: Suporte a filtros por termo de busca\n- **Interface Web**: Dashboard com estat√≠sticas e links dos feeds\n\n## üìã Endpoints\n\n### Feeds\n- `GET /feeds/lance/rss.xml` - Feed RSS 2.0\n- `GET /feeds/lance/atom.xml` - Feed Atom 1.0\n\n### Utilit√°rios\n- `GET /` - P√°gina inicial com documenta√ß√£o\n- `GET /health` - Status e m√©tricas do sistema\n- `GET /admin/refresh?key=CHAVE` - Atualiza√ß√£o manual (requer chave admin)\n\n### Par√¢metros de Query\n\n| Par√¢metro | Descri√ß√£o | Padr√£o | Exemplo |\n|-----------|-----------|---------|---------|\n| `limit` | N√∫mero m√°ximo de artigos | 30 | `?limit=20` |\n| `pages` | P√°ginas para varrer | 3 | `?pages=5` |\n| `q` | Filtrar por termo | - | `?q=Flamengo` |\n| `source_url` | URL de origem alternativa | /mais-noticias | `?source_url=...` |\n| `refresh` | For√ßar nova varredura | 0 | `?refresh=1` |\n\n## üõ† Instala√ß√£o Local\n\n1. **Clone o reposit√≥rio**\n```bash\ngit clone <repository-url>\ncd lance-feed-generator\n","size_bytes":1820},"main.py":{"content":"import os\nimport logging\nfrom app.server import app\n\n# Configure logging for production\nif os.environ.get('FLASK_ENV') != 'development':\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\nif __name__ == '__main__':\n    # Development server\n    app.run(host='0.0.0.0', port=5000, debug=True)\n","size_bytes":370},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"apscheduler>=3.11.0\",\n    \"beautifulsoup4>=4.13.5\",\n    \"email-validator>=2.3.0\",\n    \"feedgen>=1.0.0\",\n    \"flask>=3.1.2\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"gunicorn>=23.0.0\",\n    \"lxml>=5.4.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"python-dateutil>=2.9.0.post0\",\n    \"requests>=2.32.5\",\n    \"tenacity>=9.1.2\",\n    \"trafilatura>=2.0.0\",\n]\n","size_bytes":487},"replit.md":{"content":"# Overview\n\nThis is a web application that generates unofficial RSS and Atom feeds for LANCE! sports news portal. The project demonstrates responsible web scraping techniques by parsing article listings, extracting metadata using JSON-LD structured data, and generating standardized RSS/Atom feeds. It includes a Flask web server with automatic background scheduling, SQLite-based article storage with deduplication, and a web dashboard for monitoring feed statistics.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Web Framework\n- **Flask-based web server** - Provides HTTP endpoints for feed generation and admin functionality\n- **Template rendering** - Uses Jinja2 templates with Bootstrap dark theme for the web interface\n- **Static file serving** - Serves CSS and other static assets\n\n## Web Scraping System\n- **Responsible scraping approach** - Respects robots.txt, implements request delays, and uses proper User-Agent headers\n- **Multi-stage extraction process** - First scrapes article listing pages, then fetches individual articles for metadata\n- **JSON-LD metadata extraction** - Parses structured data from article pages to get accurate titles, descriptions, dates, and images\n- **Pagination handling** - Follows canonical pagination links starting from `/mais-noticias` endpoint\n- **Retry mechanism** - Uses tenacity library for robust HTTP request handling with exponential backoff\n\n## Data Storage\n- **SQLite database** - Stores articles with deduplication based on URL primary key\n- **Article schema** - Tracks URL, title, description, image, author, publication dates, and fetch timestamps\n- **Performance optimization** - Uses database indexes on date fields for efficient querying\n- **Data persistence** - Creates data directory structure automatically\n\n## Feed Generation\n- **Dual format support** - Generates both RSS 2.0 and Atom 1.0 feeds using feedgen library\n- **Rich metadata** - Includes article images as enclosures with proper MIME type detection\n- **Feed customization** - Supports query parameters for limiting results, filtering by search terms, and forcing refreshes\n- **Standard compliance** - Follows RSS and Atom specifications with proper namespaces and required fields\n\n## Background Processing\n- **Automatic scheduling** - Uses APScheduler to refresh feeds every 15 minutes\n- **Concurrent safety** - Implements threading locks and max instances to prevent overlapping runs\n- **Manual refresh capability** - Provides admin endpoint for on-demand feed updates\n- **Initial data loading** - Performs startup refresh to populate feeds immediately\n\n## API Design\n- **RESTful endpoints** - Clean URL structure for feeds and utility functions\n- **Query parameter support** - Flexible filtering and pagination options\n- **Health monitoring** - Status endpoint with system metrics and statistics\n- **Admin functionality** - Protected refresh endpoint with key-based authentication\n\n# External Dependencies\n\n## Core Web Framework\n- **Flask** - Main web application framework for handling HTTP requests and responses\n- **Jinja2** - Template engine for rendering HTML pages (included with Flask)\n\n## Web Scraping and HTTP\n- **requests** - HTTP client library for fetching web pages and handling sessions\n- **BeautifulSoup4** - HTML parsing library for extracting links and content from web pages\n- **tenacity** - Retry library for robust HTTP request handling with exponential backoff\n\n## Feed Generation\n- **feedgen** - Library for generating RSS and Atom feeds with proper XML formatting\n- **python-dateutil** - Advanced date parsing library for handling various date formats\n\n## Background Processing\n- **APScheduler** - Advanced Python Scheduler for background feed refresh jobs\n\n## Data Storage\n- **sqlite3** - Built-in Python SQLite database interface (no external dependency)\n\n## Utility Libraries\n- **urllib.robotparser** - Built-in Python library for parsing and respecting robots.txt files\n- **pathlib** - Built-in Python library for file system path operations\n\n## Frontend Assets (CDN)\n- **Bootstrap** - CSS framework served from Replit CDN for responsive UI design\n- **Font Awesome** - Icon library served from CDN for visual elements\n\n## Environment and Configuration\n- **os** - Built-in Python library for environment variable access\n- **logging** - Built-in Python logging framework for application monitoring","size_bytes":4411},"app/__init__.py":{"content":"# Lance RSS/Atom Feed Generator\n# Educational web scraping project for generating feeds from LANCE! sports news\n","size_bytes":112},"app/feeds.py":{"content":"import logging\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom feedgen.feed import FeedGenerator as FG\nfrom .utils import extract_mime_type\nfrom .sources_config import SOURCES_CONFIG\n\nlogger = logging.getLogger(__name__)\n\nclass FeedGenerator:\n    def __init__(self):\n        self.base_info = {\n            'id': 'https://lance-feeds.repl.co/',\n            'title': 'LANCE! - Feed n√£o oficial',\n            'link': {'href': 'https://www.lance.com.br/', 'rel': 'alternate'},\n            'description': 'Feed RSS/Atom n√£o oficial do LANCE! - Not√≠cias de futebol e esportes',\n            'language': 'pt-BR',\n            'generator': 'Lance Feed Generator v1.0'\n        }\n    \n    def _create_base_feed(self, source='lance', section='futebol', feed_format='rss'):\n        \"\"\"Create base feed with source-specific metadata\"\"\"\n        fg = FG()\n        \n        # Get source configuration\n        source_config = SOURCES_CONFIG.get(source, SOURCES_CONFIG['lance'])\n        section_config = source_config['sections'].get(section, list(source_config['sections'].values())[0])\n        \n        # Basic feed information with unique IDs\n        unique_id = f\"https://lance-feeds.repl.co/feeds/{source}/{section}/{feed_format}\"\n        fg.id(unique_id)\n        fg.title(f\"{source_config['name']} - {section_config['name']} - Feed n√£o oficial\")\n        fg.link(href=source_config['base_url'], rel='alternate')\n        fg.description(section_config['description'])\n        fg.language(source_config.get('language', 'pt-BR'))\n        fg.generator('Multi-Source Feed Generator v1.0')\n        \n        # Additional metadata\n        fg.lastBuildDate(datetime.now(timezone.utc))\n        fg.managingEditor('noreply@lance-feeds.repl.co (Multi-Source Feed Bot)')\n        fg.webMaster('noreply@lance-feeds.repl.co (Multi-Source Feed Bot)')\n        \n        return fg\n    \n    def _add_article_to_feed(self, fg, article):\n        \"\"\"Add a single article to the feed\"\"\"\n        try:\n            fe = fg.add_entry()\n            \n            # Required fields\n            fe.id(article['url'])\n            fe.title(article['title'])\n            fe.link(href=article['url'])\n            fe.description(article['description'] or article['title'])\n            \n            # Dates\n            if article['date_published']:\n                fe.published(article['date_published'])\n            \n            if article['date_modified']:\n                fe.updated(article['date_modified'])\n            elif article['date_published']:\n                fe.updated(article['date_published'])\n            else:\n                # Ensure fetched_at has timezone info\n                fetched_at = article['fetched_at']\n                if fetched_at and hasattr(fetched_at, 'tzinfo') and fetched_at.tzinfo is None:\n                    fetched_at = fetched_at.replace(tzinfo=timezone.utc)\n                fe.updated(fetched_at)\n            \n            # Author\n            if article['author']:\n                fe.author(name=article['author'])\n            \n            # GUID (for RSS)\n            fe.guid(article['url'], permalink=True)\n            \n            # Image enclosure\n            if article['image']:\n                try:\n                    mime_type = extract_mime_type(article['image'])\n                    fe.enclosure(article['image'], length='0', type=mime_type)\n                except Exception as e:\n                    logger.warning(f\"Could not add image enclosure for {article['url']}: {e}\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error adding article to feed: {article['url']}: {e}\")\n            return False\n    \n    def generate_rss(self, articles, source='lance', section='futebol'):\n        \"\"\"Generate RSS 2.0 feed\"\"\"\n        try:\n            # Get source configuration\n            source_config = SOURCES_CONFIG.get(source, SOURCES_CONFIG['lance'])\n            section_config = source_config['sections'].get(section, list(source_config['sections'].values())[0])\n            \n            fg = self._create_base_feed(source=source, section=section, feed_format='rss')\n            \n            # RSS-specific settings\n            fg.link(href=f'https://lance-feeds.repl.co/feeds/{source}/{section}/rss', rel='self')\n            fg.ttl(15)  # 15 minutes TTL\n            \n            # Add articles (reverse order so newest appear first in feed)\n            added_count = 0\n            for article in reversed(articles):\n                if self._add_article_to_feed(fg, article):\n                    added_count += 1\n            \n            logger.info(f\"Generated RSS feed with {added_count} articles\")\n            return fg.rss_str(pretty=True).decode('utf-8')\n            \n        except Exception as e:\n            logger.error(f\"Error generating RSS feed: {e}\")\n            raise\n    \n    def generate_atom(self, articles, source='lance', section='futebol'):\n        \"\"\"Generate Atom 1.0 feed\"\"\"\n        try:\n            # Get source configuration\n            source_config = SOURCES_CONFIG.get(source, SOURCES_CONFIG['lance'])\n            section_config = source_config['sections'].get(section, list(source_config['sections'].values())[0])\n            \n            fg = self._create_base_feed(source=source, section=section, feed_format='atom')\n            \n            # Atom-specific settings\n            fg.link(href=f'https://lance-feeds.repl.co/feeds/{source}/{section}/atom', rel='self')\n            fg.updated(datetime.now(timezone.utc))\n            \n            # Add articles (reverse order so newest appear first in feed)\n            added_count = 0\n            for article in reversed(articles):\n                if self._add_article_to_feed(fg, article):\n                    added_count += 1\n            \n            logger.info(f\"Generated Atom feed with {added_count} articles\")\n            return fg.atom_str(pretty=True).decode('utf-8')\n            \n        except Exception as e:\n            logger.error(f\"Error generating Atom feed: {e}\")\n            raise\n","size_bytes":6070},"app/scheduler.py":{"content":"import logging\nimport threading\nfrom datetime import datetime\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.interval import IntervalTrigger\n\nlogger = logging.getLogger(__name__)\n\nclass FeedScheduler:\n    def __init__(self, scraper, store, refresh_interval_minutes=10):\n        self.scraper = scraper\n        self.store = store\n        self.refresh_interval_minutes = refresh_interval_minutes\n        self.scheduler = BackgroundScheduler()\n        self.is_running_flag = False\n        self.last_run = None\n        self.lock = threading.Lock()\n\n    def start(self):\n        \"\"\"Start the background scheduler\"\"\"\n        try:\n            # Add the refresh job\n            self.scheduler.add_job(\n                func=self._refresh_job,\n                trigger=IntervalTrigger(minutes=self.refresh_interval_minutes),\n                id='feed_refresh',\n                name='Feed Refresh Job',\n                replace_existing=True,\n                max_instances=1  # Prevent overlapping runs\n            )\n\n            self.scheduler.start()\n            logger.info(f\"Scheduler started - refresh every {self.refresh_interval_minutes} minutes\")\n\n            # Run initial refresh in background\n            self.scheduler.add_job(\n                func=self._initial_refresh,\n                trigger='date',\n                run_date=datetime.now(),\n                id='initial_refresh',\n                name='Initial Refresh',\n                max_instances=1\n            )\n\n        except Exception as e:\n            logger.error(f\"Error starting scheduler: {e}\")\n\n    def stop(self):\n        \"\"\"Stop the background scheduler\"\"\"\n        try:\n            if self.scheduler.running:\n                self.scheduler.shutdown()\n                logger.info(\"Scheduler stopped\")\n        except Exception as e:\n            logger.error(f\"Error stopping scheduler: {e}\")\n\n    def is_running(self):\n        \"\"\"Check if scheduler is running\"\"\"\n        return self.scheduler.running if self.scheduler else False\n\n    def _initial_refresh(self):\n        \"\"\"Initial refresh on startup\"\"\"\n        try:\n            # Check if we have any articles in the database\n            stats = self.store.get_stats()\n\n            if stats['total_articles'] == 0:\n                logger.info(\"No articles in database, performing initial scrape\")\n                self._refresh_job()\n            else:\n                logger.info(f\"Database has {stats['total_articles']} articles, skipping initial scrape\")\n\n        except Exception as e:\n            logger.error(f\"Error in initial refresh: {e}\")\n\n    def _refresh_job(self):\n        \"\"\"Background job to refresh feeds from multiple sources\"\"\"\n        with self.lock:\n            if self.is_running_flag:\n                logger.warning(\"Refresh job already running, skipping\")\n                return\n\n            self.is_running_flag = True\n\n        try:\n            logger.info(\"Starting multi-source feed refresh\")\n            start_time = datetime.utcnow()\n\n            # Refresh all configured LANCE! sections\n            total_new_articles = 0\n            source = 'lance'\n\n            from .sources_config import SOURCES_CONFIG\n            from .scraper_factory import ScraperFactory\n            \n            source_config = SOURCES_CONFIG.get(source, {})\n            sections = source_config.get('sections', {}).keys()\n\n            for section in sections:\n                try:\n                    logger.info(f\"Refreshing {source}/{section}\")\n                    new_articles = ScraperFactory.scrape_source_section(\n                        source, section, self.store, \n                        max_pages=1, max_articles=5, request_delay=0.5\n                    )\n                    total_new_articles += len(new_articles)\n                    logger.info(f\"Added {len(new_articles)} new articles for {source}/{section}\")\n\n                    # Small delay between sections to be nice to servers\n                    time.sleep(2)\n\n                except Exception as e:\n                    logger.error(f\"Error refreshing {source}/{section}: {e}\")\n                    continue\n\n            # Update last run time\n            self.last_run = datetime.utcnow()\n\n            # Log results\n            duration = (self.last_run - start_time).total_seconds()\n            logger.info(f\"Multi-source feed refresh completed in {duration:.1f}s - {total_new_articles} new articles\")\n\n            # Optional: cleanup old articles (keep last 30 days)\n            try:\n                deleted_count = self.store.cleanup_old_articles(days_to_keep=30)\n                if deleted_count > 0:\n                    logger.info(f\"Cleaned up {deleted_count} old articles\")\n            except Exception as e:\n                logger.warning(f\"Error during cleanup: {e}\")\n\n        except Exception as e:\n            logger.error(f\"Error in refresh job: {e}\")\n        finally:\n            with self.lock:\n                self.is_running_flag = False\n\n    def trigger_refresh(self):\n        \"\"\"Manually trigger a refresh (for admin endpoint)\"\"\"\n        try:\n            # Add a one-time job\n            self.scheduler.add_job(\n                func=self._refresh_job,\n                trigger='date',\n                run_date=datetime.now(),\n                id=f'manual_refresh_{datetime.now().timestamp()}',\n                name='Manual Refresh',\n                max_instances=1\n            )\n            return True\n        except Exception as e:\n            logger.error(f\"Error triggering manual refresh: {e}\")\n            return False\n\n    def get_status(self):\n        \"\"\"Get scheduler status information\"\"\"\n        return {\n            'running': self.is_running(),\n            'refresh_interval_minutes': self.refresh_interval_minutes,\n            'last_run': self.last_run.isoformat() if self.last_run else None,\n            'is_job_running': self.is_running_flag,\n            'next_run': self._get_next_run_time()\n        }\n\n    def _get_next_run_time(self):\n        \"\"\"Get next scheduled run time\"\"\"\n        try:\n            job = self.scheduler.get_job('feed_refresh')\n            if job and job.next_run_time:\n                return job.next_run_time.isoformat()\n        except Exception:\n            pass\n        return None","size_bytes":6275},"app/scraper.py":{"content":"import re\nimport json\nimport time\nimport urllib.robotparser\nfrom urllib.parse import urljoin, urlparse\nfrom datetime import datetime\nimport logging\nimport requests\nfrom bs4 import BeautifulSoup\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom dateutil import parser as date_parser\nfrom .utils import normalize_date, extract_mime_type, get_user_agent\n\nlogger = logging.getLogger(__name__)\n\ndef clean_image_url(url: str) -> str:\n    \"\"\"Clean image URL by removing CDN optimization parameters\"\"\"\n    if not url:\n        return url\n\n    if \"/uploads/\" in url:\n        # Extract the host from the original URL\n        if url.startswith('http'):\n            from urllib.parse import urlparse\n            parsed = urlparse(url)\n            host = f\"{parsed.scheme}://{parsed.netloc}\"\n        else:\n            host = \"https://lncimg.lance.com.br\"\n\n        # Keep the host + path from /uploads/\n        uploads_part = url.split(\"/uploads/\", 1)[-1]\n        return f\"{host}/uploads/{uploads_part}\"\n\n    return url\n\nclass LanceScraper:\n    def __init__(self, store, request_delay=1.0):\n        self.store = store\n        self.request_delay = request_delay\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': get_user_agent()\n        })\n        self.robots_checker = None\n\n    def _check_robots_txt(self, url):\n        \"\"\"Check if we can fetch the given URL according to robots.txt\"\"\"\n        try:\n            if not self.robots_checker:\n                base_url = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n                robots_url = urljoin(base_url, '/robots.txt')\n\n                self.robots_checker = urllib.robotparser.RobotFileParser()\n                self.robots_checker.set_url(robots_url)\n                self.robots_checker.read()\n\n            # Debug the robots check\n            user_agent = get_user_agent()\n            can_fetch = self.robots_checker.can_fetch(user_agent, url)\n            logger.debug(f\"Robots check for {url} with UA '{user_agent}': {can_fetch}\")\n\n            # LANCE! robots.txt allows all (Allow: /), so if we get False, there's a parsing issue\n            # Let's be more permissive for educational purposes\n            if not can_fetch and 'lance.com.br' in url:\n                logger.info(f\"Robots parser returned False for LANCE!, but robots.txt allows all. Proceeding.\")\n                return True\n\n            return can_fetch\n        except Exception as e:\n            logger.warning(f\"Could not check robots.txt: {e}\")\n            return True  # Assume allowed if we can't check\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def _fetch_page(self, url):\n        \"\"\"Fetch a web page with retries and error handling\"\"\"\n        if not self._check_robots_txt(url):\n            logger.warning(f\"robots.txt disallows fetching {url}\")\n            return None\n\n        try:\n            response = self.session.get(url, timeout=30)\n            response.raise_for_status()\n            return response.text\n        except requests.RequestException as e:\n            logger.error(f\"Failed to fetch {url}: {e}\")\n            raise\n\n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from listing page HTML\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        links = []\n\n        # Find all links that point to articles (ending in .html)\n        for link in soup.find_all('a', href=True):\n            href = link.get('href', '')\n\n            # Convert relative URLs to absolute\n            if href and isinstance(href, str) and href.startswith('/'):\n                href = urljoin(base_url, href)\n\n            # Filter for LANCE articles ending in .html\n            if (href and isinstance(href, str) and href.startswith('https://www.lance.com.br/') and \n                href.endswith('.html')):\n                links.append(href)\n\n        # Remove duplicates while preserving order\n        seen = set()\n        unique_links = []\n        for link in links:\n            if link not in seen:\n                seen.add(link)\n                unique_links.append(link)\n\n        logger.info(f\"Extracted {len(unique_links)} article links from {base_url}\")\n        return unique_links\n\n    def find_next_page(self, html, current_url):\n        \"\"\"Find the next page URL from pagination\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n\n        # Look for <link rel=\"next\">\n        next_link = soup.find('link', {'rel': 'next'})\n        if next_link and hasattr(next_link, 'get') and next_link.get('href'):\n            next_url = next_link.get('href', '')\n            if next_url and isinstance(next_url, str) and next_url.startswith('/'):\n                next_url = urljoin(current_url, next_url)\n            return next_url\n\n        return None\n\n    def list_pages(self, start_url, max_pages=3):\n        \"\"\"Collect article links from paginated listing\"\"\"\n        all_links = []\n        current_url = start_url\n        page_count = 0\n\n        logger.info(f\"Starting pagination from {start_url}, max_pages={max_pages}\")\n\n        while current_url and page_count < max_pages:\n            try:\n                logger.info(f\"Fetching page {page_count + 1}: {current_url}\")\n\n                html = self._fetch_page(current_url)\n                if not html:\n                    break\n\n                # Extract article links\n                page_links = self.extract_article_links(html, current_url)\n                all_links.extend(page_links)\n\n                # Find next page\n                next_url = self.find_next_page(html, current_url)\n                current_url = next_url\n                page_count += 1\n\n                # Rate limiting\n                if current_url:\n                    time.sleep(self.request_delay)\n\n            except Exception as e:\n                logger.error(f\"Error processing page {current_url}: {e}\")\n                break\n\n        logger.info(f\"Collected {len(all_links)} total links from {page_count} pages\")\n        return all_links\n\n    def parse_json_ld(self, html):\n        \"\"\"Extract JSON-LD metadata from article page\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n\n        # Find all JSON-LD scripts\n        scripts = soup.find_all('script', {'type': 'application/ld+json'})\n\n        for script in scripts:\n            try:\n                script_content = getattr(script, 'string', None) or ''\n                if not script_content or not isinstance(script_content, str):\n                    continue\n                data = json.loads(script_content)\n\n                # Handle different JSON-LD structures\n                items = []\n\n                if isinstance(data, dict):\n                    if '@graph' in data:\n                        items = data['@graph']\n                    else:\n                        items = [data]\n                elif isinstance(data, list):\n                    items = data\n\n                # Look for Article/NewsArticle/BlogPosting\n                for item in items:\n                    if isinstance(item, dict):\n                        item_type = item.get('@type', '')\n                        if item_type in ['Article', 'NewsArticle', 'BlogPosting']:\n                            return self._extract_article_metadata(item)\n\n            except (json.JSONDecodeError, KeyError) as e:\n                logger.warning(f\"Error parsing JSON-LD: {e}\")\n                continue\n\n        return None\n\n    def _extract_article_metadata(self, json_ld):\n        \"\"\"Extract article metadata from JSON-LD object\"\"\"\n        metadata = {}\n\n        # Title/headline\n        metadata['title'] = json_ld.get('headline', json_ld.get('name', ''))\n\n        # Description\n        metadata['description'] = json_ld.get('description', '')\n\n        # Image\n        image = json_ld.get('image')\n        if image:\n            raw_image_url = ''\n            if isinstance(image, str):\n                raw_image_url = image\n            elif isinstance(image, dict):\n                raw_image_url = image.get('url', '')\n            elif isinstance(image, list) and len(image) > 0:\n                if isinstance(image[0], str):\n                    raw_image_url = image[0]\n                elif isinstance(image[0], dict):\n                    raw_image_url = image[0].get('url', '')\n\n            # Clean the image URL\n            metadata['image'] = clean_image_url(raw_image_url)\n\n        # Dates\n        metadata['date_published'] = json_ld.get('datePublished', '')\n        metadata['date_modified'] = json_ld.get('dateModified', '')\n\n        # Author\n        author = json_ld.get('author')\n        if author:\n            if isinstance(author, str):\n                metadata['author'] = author\n            elif isinstance(author, dict):\n                metadata['author'] = author.get('name', '')\n            elif isinstance(author, list) and len(author) > 0:\n                if isinstance(author[0], str):\n                    metadata['author'] = author[0]\n                elif isinstance(author[0], dict):\n                    metadata['author'] = author[0].get('name', '')\n\n        return metadata\n\n    def extract_fallback_metadata(self, html, url):\n        \"\"\"Extract fallback metadata from HTML meta tags\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        metadata = {}\n\n        # Title fallback\n        title_tag = soup.find('title')\n        if title_tag:\n            metadata['title'] = title_tag.get_text().strip()\n\n        # Description fallback\n        desc_meta = soup.find('meta', {'name': 'description'}) or \\\n                   soup.find('meta', {'property': 'og:description'})\n        if desc_meta and hasattr(desc_meta, 'get') and desc_meta.get('content'):\n            metadata['description'] = str(desc_meta.get('content', ''))\n\n        # Image fallback\n        img_meta = soup.find('meta', {'property': 'og:image'})\n        if img_meta and hasattr(img_meta, 'get') and img_meta.get('content'):\n            raw_image_url = str(img_meta.get('content', ''))\n            metadata['image'] = clean_image_url(raw_image_url)\n\n        return metadata\n\n    def parse_article(self, url):\n        \"\"\"Parse individual article and extract metadata\"\"\"\n        try:\n            html = self._fetch_page(url)\n            if not html:\n                return None\n\n            # Try JSON-LD first\n            metadata = self.parse_json_ld(html)\n\n            # Fallback to HTML meta tags\n            if not metadata or not metadata.get('title'):\n                fallback = self.extract_fallback_metadata(html, url)\n                if not metadata:\n                    metadata = fallback\n                else:\n                    # Merge fallback data for missing fields\n                    for key, value in fallback.items():\n                        if not metadata.get(key):\n                            metadata[key] = value\n\n            if not metadata:\n                return None\n\n            # Normalize the article data\n            article = {\n                'url': url,\n                'title': metadata.get('title', '').strip(),\n                'description': metadata.get('description', '').strip(),\n                'image': clean_image_url(metadata.get('image', '').strip()),\n                'author': metadata.get('author', '').strip(),\n                'date_published': normalize_date(metadata.get('date_published')),\n                'date_modified': normalize_date(metadata.get('date_modified')),\n                'fetched_at': datetime.utcnow()\n            }\n\n            # Skip articles without title\n            if not article['title']:\n                logger.warning(f\"Skipping article without title: {url}\")\n                return None\n\n            return article\n\n        except Exception as e:\n            logger.error(f\"Error parsing article {url}: {e}\")\n            return None\n\n    def scrape_and_store(self, start_urls, max_pages=3):\n        \"\"\"Main scraping method that collects and stores articles\"\"\"\n        # Handle both single URL and list of URLs\n        if isinstance(start_urls, str):\n            start_urls = [start_urls]\n\n        logger.info(f\"Starting scrape from {len(start_urls)} source(s): {start_urls}\")\n\n        # Get article links from all listing pages\n        all_article_urls = []\n        for start_url in start_urls:\n            logger.info(f\"Scraping from: {start_url}\")\n            article_urls = self.list_pages(start_url, max_pages)\n            all_article_urls.extend(article_urls)\n\n        # Remove duplicates while preserving order\n        seen = set()\n        article_urls = []\n        for url in all_article_urls:\n            if url not in seen:\n                seen.add(url)\n                article_urls.append(url)\n\n        if not article_urls:\n            logger.warning(\"No article URLs found\")\n            return []\n\n        # Process each article\n        new_articles = []\n        for i, url in enumerate(article_urls):\n            try:\n                logger.info(f\"Processing article {i+1}/{len(article_urls)}: {url}\")\n\n                # Check if we already have this article\n                if self.store.has_article(url):\n                    logger.debug(f\"Article already exists: {url}\")\n                    continue\n\n                # Parse article\n                article = self.parse_article(url)\n                if article:\n                    # Store in database\n                    self.store.upsert_article(article)\n                    new_articles.append(article)\n                    logger.info(f\"Stored article: {article['title']}\")\n\n                # Rate limiting between articles\n                time.sleep(self.request_delay)\n\n            except Exception as e:\n                logger.error(f\"Error processing article {url}: {e}\")\n                continue\n\n        logger.info(f\"Scraping completed. New articles: {len(new_articles)}\")\n        return new_articles","size_bytes":13886},"app/server.py":{"content":"import os\nimport logging\nimport json\nfrom datetime import datetime\nfrom flask import Flask, request, jsonify, render_template, Response\nfrom .scraper import LanceScraper\nfrom .feeds import FeedGenerator\nfrom .store import ArticleStore\nfrom .scheduler import FeedScheduler\nfrom .utils import validate_admin_key, parse_query_filter\nfrom .sources_config import SOURCES_CONFIG as SOURCES\nfrom .scraper_factory import ScraperFactory\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize Flask app\napp = Flask(__name__, template_folder='../templates', static_folder='../static')\napp.secret_key = os.environ.get(\"SESSION_SECRET\")\nif not app.secret_key:\n    # For development only - use proper env var in production\n    import secrets\n    app.secret_key = secrets.token_hex(32)\n    logger.warning(\"SESSION_SECRET not set, using temporary secret key for development\")\n\n# Configuration from environment\nMAX_PAGES = int(os.environ.get(\"MAX_PAGES\", \"3\"))\nDEFAULT_LIMIT = int(os.environ.get(\"DEFAULT_LIMIT\", \"30\"))\nREQUEST_DELAY_MS = int(os.environ.get(\"REQUEST_DELAY_MS\", \"900\"))\nADMIN_KEY = os.environ.get(\"ADMIN_KEY\", \"\")\n\n# Initialize components\nstore = ArticleStore()\nscraper = LanceScraper(store, request_delay=REQUEST_DELAY_MS/1000.0)\nfeed_generator = FeedGenerator()\nscheduler = FeedScheduler(scraper, store)\n\n# Start background scheduler\nscheduler.start()\n\n@app.route('/')\ndef index():\n    \"\"\"Landing page with feed information and usage examples\"\"\"\n    try:\n        stats = store.get_stats()\n        return render_template('index.html', stats=stats)\n    except Exception as e:\n        logger.error(f\"Error loading index page: {e}\")\n        return render_template('index.html', stats={'total_articles': 0, 'last_update': None})\n\n@app.route('/feeds/<source>/<section>/<format>')\ndef dynamic_feeds(source, section, format):\n    \"\"\"Dynamic feeds for any source/section/format combination\"\"\"\n    try:\n        # Validate source and section\n        if source not in SOURCES:\n            return f\"Unknown source: {source}\", 404\n        \n        if section not in SOURCES[source]['sections']:\n            return f\"Unknown section '{section}' for source '{source}'\", 404\n        \n        # Validate format\n        if format not in ['rss', 'atom']:\n            return f\"Unsupported format: {format}. Use 'rss' or 'atom'\", 400\n        \n        # Get parameters\n        limit = min(int(request.args.get('limit', DEFAULT_LIMIT)), 100)\n        query = request.args.get('q', '')\n        force_refresh = request.args.get('refresh') == '1'\n        \n        # Get section-specific filters\n        section_config = SOURCES[source]['sections'][section]\n        exclude_authors = section_config.get('filters', {}).get('exclude_authors', [])\n        \n        # Get articles with filters\n        query_filter = parse_query_filter(query) if query else None\n        articles = store.get_recent_articles(\n            limit=limit, \n            query_filter=query_filter,\n            source=source,\n            section=section,\n            exclude_authors=exclude_authors\n        )\n        \n        # Force refresh if requested and no recent articles\n        if force_refresh or not articles:\n            logger.info(f\"Force refresh requested for {source}/{section}\")\n            # Use ScraperFactory to scrape any source\n            new_articles = ScraperFactory.scrape_source_section(\n                source=source, \n                section=section, \n                store=store, \n                max_pages=2,  # Reduced for performance\n                max_articles=20,  # Limit articles to prevent timeouts\n                request_delay=0.3  # Reduced delay for faster scraping\n            )\n            logger.info(f\"Scraped {len(new_articles)} new articles for {source}/{section}\")\n            articles = store.get_recent_articles(\n                limit=limit, \n                query_filter=query_filter,\n                source=source,\n                section=section,\n                exclude_authors=exclude_authors\n            )\n        \n        # Generate feed\n        if format == 'rss':\n            feed_content = feed_generator.generate_rss(articles, source=source, section=section)\n            content_type = 'application/rss+xml'\n        else:  # atom\n            feed_content = feed_generator.generate_atom(articles, source=source, section=section)\n            content_type = 'application/atom+xml'\n        \n        response = Response(feed_content, mimetype=content_type)\n        response.headers['Cache-Control'] = 'public, max-age=900'  # 15 minutes cache\n        return response\n    \n    except Exception as e:\n        logger.error(f\"Error generating {format} feed for {source}/{section}: {e}\")\n        return jsonify({'error': f'Failed to generate {format} feed'}), 500\n\n# New routes for individual category feeds\n@app.route('/feeds/lance/<section>/rss')\ndef lance_category_rss(section):\n    \"\"\"Generate RSS feed for specific LANCE! category\"\"\"\n    try:\n        # Validate section exists\n        from .sources_config import validate_source_section\n        if not validate_source_section('lance', section):\n            return jsonify({'error': f'Unknown section: {section}'}), 404\n        \n        limit = min(int(request.args.get('limit', DEFAULT_LIMIT)), 100)\n        force_refresh = request.args.get('refresh') == '1'\n        \n        logger.info(f\"RSS feed requested for lance/{section}: limit={limit}, refresh={force_refresh}\")\n        \n        # Get articles from database for this section\n        articles = store.get_recent_articles(limit=limit, source='lance', section=section)\n        \n        # Force refresh if requested or no articles\n        if force_refresh or not articles:\n            logger.info(f\"Force refresh requested for lance/{section}\")\n            new_articles = ScraperFactory.scrape_source_section(\n                'lance', section, store, max_pages=2, max_articles=20, request_delay=0.3\n            )\n            logger.info(f\"Scraped {len(new_articles)} new articles for lance/{section}\")\n            articles = store.get_recent_articles(limit=limit, source='lance', section=section)\n        \n        # Generate RSS feed\n        rss_content = feed_generator.generate_rss(articles, source='lance', section=section)\n        \n        response = Response(rss_content, mimetype='application/rss+xml')\n        response.headers['Cache-Control'] = 'public, max-age=900'\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Error generating RSS feed for lance/{section}: {e}\")\n        return jsonify({'error': f'Failed to generate RSS feed for {section}'}), 500\n\n@app.route('/feeds/lance/<section>/atom')\ndef lance_category_atom(section):\n    \"\"\"Generate Atom feed for specific LANCE! category\"\"\"\n    try:\n        # Validate section exists\n        from .sources_config import validate_source_section\n        if not validate_source_section('lance', section):\n            return jsonify({'error': f'Unknown section: {section}'}), 404\n        \n        limit = min(int(request.args.get('limit', DEFAULT_LIMIT)), 100)\n        force_refresh = request.args.get('refresh') == '1'\n        \n        logger.info(f\"Atom feed requested for lance/{section}: limit={limit}, refresh={force_refresh}\")\n        \n        # Get articles from database for this section\n        articles = store.get_recent_articles(limit=limit, source='lance', section=section)\n        \n        # Force refresh if requested or no articles\n        if force_refresh or not articles:\n            logger.info(f\"Force refresh requested for lance/{section}\")\n            new_articles = ScraperFactory.scrape_source_section(\n                'lance', section, store, max_pages=2, max_articles=20, request_delay=0.3\n            )\n            logger.info(f\"Scraped {len(new_articles)} new articles for lance/{section}\")\n            articles = store.get_recent_articles(limit=limit, source='lance', section=section)\n        \n        # Generate Atom feed\n        atom_content = feed_generator.generate_atom(articles, source='lance', section=section)\n        \n        response = Response(atom_content, mimetype='application/atom+xml')\n        response.headers['Cache-Control'] = 'public, max-age=900'\n        return response\n        \n    except Exception as e:\n        logger.error(f\"Error generating Atom feed for lance/{section}: {e}\")\n        return jsonify({'error': f'Failed to generate Atom feed for {section}'}), 500\n\n# Legacy routes for backward compatibility\n@app.route('/feeds/lance/rss.xml')\ndef rss_feed():\n    \"\"\"Generate RSS feed\"\"\"\n    try:\n        # Parse query parameters\n        limit = min(int(request.args.get('limit', DEFAULT_LIMIT)), 100)\n        pages = min(int(request.args.get('pages', MAX_PAGES)), 10)\n        q = request.args.get('q', '')\n        # Fixed source URL for security (no user-controlled URLs)\n        source_url = 'https://www.lance.com.br/mais-noticias'\n\n        logger.info(f\"RSS feed requested: limit={limit}, pages={pages}, q='{q}'\")\n\n        # Get articles from database\n        query_filter = parse_query_filter(q) if q else None\n        articles = store.get_recent_articles(limit=limit, query_filter=query_filter)\n\n        # If no recent articles or forced refresh, scrape new content\n        if not articles or request.args.get('refresh') == '1':\n            logger.info(\"Triggering fresh scrape for RSS feed\")\n            new_articles = scraper.scrape_and_store(source_url, max_pages=pages)\n            articles = store.get_recent_articles(limit=limit, query_filter=query_filter)\n\n        # Generate RSS feed\n        rss_content = feed_generator.generate_rss(articles)\n\n        response = Response(rss_content, mimetype='application/rss+xml')\n        response.headers['Cache-Control'] = 'public, max-age=900'  # 15 minutes cache\n        return response\n\n    except Exception as e:\n        logger.error(f\"Error generating RSS feed: {e}\")\n        return jsonify({'error': 'Failed to generate RSS feed'}), 500\n\n@app.route('/feeds/lance/atom.xml')\ndef atom_feed():\n    \"\"\"Generate Atom feed\"\"\"\n    try:\n        # Parse query parameters\n        limit = min(int(request.args.get('limit', DEFAULT_LIMIT)), 100)\n        pages = min(int(request.args.get('pages', MAX_PAGES)), 10)\n        q = request.args.get('q', '')\n        # Fixed source URL for security\n        source_url = 'https://www.lance.com.br/mais-noticias'\n\n        logger.info(f\"Atom feed requested: limit={limit}, pages={pages}, q='{q}'\")\n\n        # Get articles from database\n        query_filter = parse_query_filter(q) if q else None\n        articles = store.get_recent_articles(limit=limit, query_filter=query_filter)\n\n        # If no recent articles or forced refresh, scrape new content\n        if not articles or request.args.get('refresh') == '1':\n            logger.info(\"Triggering fresh scrape for Atom feed\")\n            new_articles = scraper.scrape_and_store(source_url, max_pages=pages)\n            articles = store.get_recent_articles(limit=limit, query_filter=query_filter)\n\n        # Generate Atom feed\n        atom_content = feed_generator.generate_atom(articles)\n\n        response = Response(atom_content, mimetype='application/atom+xml')\n        response.headers['Cache-Control'] = 'public, max-age=900'  # 15 minutes cache\n        return response\n\n    except Exception as e:\n        logger.error(f\"Error generating Atom feed: {e}\")\n        return jsonify({'error': 'Failed to generate Atom feed'}), 500\n\n@app.route('/health')\ndef health_check():\n    \"\"\"Health check endpoint with metrics\"\"\"\n    try:\n        stats = store.get_stats()\n        return jsonify({\n            'status': 'ok',\n            'timestamp': datetime.utcnow().isoformat(),\n            'items': stats['total_articles'],\n            'last_refresh': stats['last_update'],\n            'scheduler_running': scheduler.is_running()\n        })\n    except Exception as e:\n        logger.error(f\"Health check error: {e}\")\n        return jsonify({\n            'status': 'error',\n            'error': str(e),\n            'timestamp': datetime.utcnow().isoformat()\n        }), 500\n\n@app.route('/admin/refresh')\ndef admin_refresh():\n    \"\"\"Manual refresh endpoint (requires admin key)\"\"\"\n    try:\n        # Validate admin key\n        provided_key = request.args.get('key', '')\n        if not validate_admin_key(provided_key, ADMIN_KEY):\n            return jsonify({'error': 'Invalid admin key'}), 401\n\n        # Parse parameters\n        pages = min(int(request.args.get('pages', MAX_PAGES)), 10)\n        # Fixed source URL for security\n        source_url = 'https://www.lance.com.br/mais-noticias'\n\n        logger.info(f\"Manual refresh triggered: pages={pages}\")\n\n        # Perform scraping\n        new_articles = scraper.scrape_and_store(source_url, max_pages=pages)\n        stats = store.get_stats()\n\n        return jsonify({\n            'status': 'success',\n            'new_articles': len(new_articles),\n            'total_articles': stats['total_articles'],\n            'timestamp': datetime.utcnow().isoformat()\n        })\n\n    except Exception as e:\n        logger.error(f\"Admin refresh error: {e}\")\n        return jsonify({\n            'status': 'error',\n            'error': str(e),\n            'timestamp': datetime.utcnow().isoformat()\n        }), 500\n\n@app.route('/admin/stats')\ndef admin_stats():\n    \"\"\"Detailed statistics endpoint\"\"\"\n    try:\n        provided_key = request.args.get('key', '')\n        if not validate_admin_key(provided_key, ADMIN_KEY):\n            return jsonify({'error': 'Invalid admin key'}), 401\n\n        detailed_stats = store.get_detailed_stats()\n        return jsonify(detailed_stats)\n\n    except Exception as e:\n        logger.error(f\"Admin stats error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.errorhandler(404)\ndef not_found(error):\n    return jsonify({'error': 'Endpoint not found'}), 404\n\n@app.errorhandler(500)\ndef internal_error(error):\n    logger.error(f\"Internal server error: {error}\")\n    return jsonify({'error': 'Internal server error'}), 500\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)","size_bytes":14191},"app/store.py":{"content":"import sqlite3\nimport logging\nimport re\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\nclass ArticleStore:\n    def __init__(self, db_path='data/app.db'):\n        self.db_path = db_path\n        self._ensure_data_dir()\n        self._init_db()\n\n    def _ensure_data_dir(self):\n        \"\"\"Ensure the data directory exists\"\"\"\n        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)\n\n    def _init_db(self):\n        \"\"\"Initialize the database with required tables\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute('''\n                    CREATE TABLE IF NOT EXISTS articles (\n                        url TEXT PRIMARY KEY,\n                        title TEXT NOT NULL,\n                        description TEXT,\n                        image TEXT,\n                        author TEXT,\n                        date_published TIMESTAMP,\n                        date_modified TIMESTAMP,\n                        fetched_at TIMESTAMP NOT NULL,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        source TEXT DEFAULT 'lance',\n                        section TEXT DEFAULT 'futebol',\n                        site TEXT DEFAULT 'lance.com.br'\n                    )\n                ''')\n\n                # Add new columns to existing table if they don't exist\n                try:\n                    conn.execute('ALTER TABLE articles ADD COLUMN source TEXT DEFAULT \"lance\"')\n                except:\n                    pass  # Column already exists\n\n                try:\n                    conn.execute('ALTER TABLE articles ADD COLUMN section TEXT DEFAULT \"futebol\"')\n                except:\n                    pass  # Column already exists\n\n                try:\n                    conn.execute('ALTER TABLE articles ADD COLUMN site TEXT DEFAULT \"lance.com.br\"')\n                except:\n                    pass  # Column already exists\n\n                # Create indexes for performance (after columns exist)\n                conn.execute('''\n                    CREATE INDEX IF NOT EXISTS idx_articles_date_published \n                    ON articles(date_published DESC)\n                ''')\n\n                conn.execute('''\n                    CREATE INDEX IF NOT EXISTS idx_articles_fetched_at \n                    ON articles(fetched_at DESC)\n                ''')\n\n                conn.execute('''\n                    CREATE INDEX IF NOT EXISTS idx_articles_source_section \n                    ON articles(source, section, COALESCE(date_published, fetched_at) DESC)\n                ''')\n\n                conn.execute('''\n                    CREATE INDEX IF NOT EXISTS idx_articles_author \n                    ON articles(author)\n                ''')\n\n                conn.commit()\n                logger.info(\"Database initialized successfully\")\n\n        except Exception as e:\n            logger.error(f\"Error initializing database: {e}\")\n            raise\n\n    def _row_to_dict(self, row):\n        \"\"\"Convert database row to dictionary\"\"\"\n        if not row:\n            return None\n\n        # Handle both old and new schema\n        result = {\n            'url': row[0],\n            'title': row[1],\n            'description': row[2],\n            'image': row[3],\n            'author': row[4],\n            'date_published': datetime.fromisoformat(row[5]) if row[5] else None,\n            'date_modified': datetime.fromisoformat(row[6]) if row[6] else None,\n            'fetched_at': datetime.fromisoformat(row[7]) if row[7] else None,\n            'created_at': datetime.fromisoformat(row[8]) if row[8] else None,\n            'updated_at': datetime.fromisoformat(row[9]) if row[9] else None\n        }\n\n        # Add new fields if they exist in the row\n        if len(row) > 10:\n            result['source'] = row[10] or 'lance'\n            result['section'] = row[11] or 'futebol'\n            result['site'] = row[12] or 'lance.com.br'\n        else:\n            # Default values for backward compatibility\n            result['source'] = 'lance'\n            result['section'] = 'futebol'\n            result['site'] = 'lance.com.br'\n\n        return result\n\n    def has_article(self, url):\n        \"\"\"Check if article already exists in database\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute('SELECT 1 FROM articles WHERE url = ?', (url,))\n                return cursor.fetchone() is not None\n        except Exception as e:\n            logger.error(f\"Error checking article existence: {e}\")\n            return False\n\n    def upsert_article(self, article):\n        \"\"\"Insert or update article in database\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                # Convert datetime objects to ISO strings\n                date_published = article['date_published'].isoformat() if article['date_published'] else None\n                date_modified = article['date_modified'].isoformat() if article['date_modified'] else None\n                fetched_at = article['fetched_at'].isoformat()\n\n                # Check if new columns exist\n                cursor = conn.execute(\"PRAGMA table_info(articles)\")\n                columns = [row[1] for row in cursor.fetchall()]\n\n                if 'source' in columns:\n                    conn.execute('''\n                        INSERT OR REPLACE INTO articles \n                        (url, title, description, image, author, date_published, date_modified, fetched_at, updated_at, source, section, site)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, ?, ?, ?)\n                    ''', (\n                        article['url'],\n                        article['title'],\n                        article['description'],\n                        article['image'],\n                        article['author'],\n                        date_published,\n                        date_modified,\n                        fetched_at,\n                        article.get('source', 'lance'),\n                        article.get('section', 'futebol'),\n                        article.get('site', 'lance.com.br')\n                    ))\n                else:\n                    # Use old schema\n                    conn.execute('''\n                        INSERT OR REPLACE INTO articles \n                        (url, title, description, image, author, date_published, date_modified, fetched_at, updated_at)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)\n                    ''', (\n                        article['url'],\n                        article['title'],\n                        article['description'],\n                        article['image'],\n                        article['author'],\n                        date_published,\n                        date_modified,\n                        fetched_at\n                    ))\n\n                conn.commit()\n                logger.debug(f\"Article upserted: {article['url']}\")\n                return True\n\n        except Exception as e:\n            logger.error(f\"Error upserting article {article['url']}: {e}\")\n            return False\n\n    def get_recent_articles(self, limit=30, hours=24, query_filter=None, source=None, section=None, exclude_authors=None):\n        \"\"\"Get recent articles from the database with optional source/section filtering\"\"\"\n        try:\n            cutoff_time = datetime.utcnow() - timedelta(hours=hours)\n\n            # Build base WHERE conditions\n            where_conditions = [\"fetched_at >= ?\"]\n            params = [cutoff_time]\n\n            # Add source filter if provided\n            if source:\n                where_conditions.append(\"source = ?\")\n                params.append(source)\n\n            # Add section filter if provided  \n            if section:\n                where_conditions.append(\"section = ?\")\n                params.append(section)\n\n            # Add exclude_authors filter if provided\n            if exclude_authors:\n                # Create NOT IN condition for excluded authors\n                placeholders = ','.join(['?' for _ in exclude_authors])\n                where_conditions.append(f\"(author IS NULL OR author NOT IN ({placeholders}))\")\n                params.extend(exclude_authors)\n\n            if query_filter:\n                # Build search query\n                search_terms = query_filter.get('terms', [])\n                if search_terms:\n                    # Create LIKE conditions for title and description\n                    like_conditions = []\n                    for term in search_terms:\n                        like_conditions.append(\"(title LIKE ? OR description LIKE ?)\")\n                        params.extend([f'%{term}%', f'%{term}%'])\n\n                    where_conditions.append(f\"({' OR '.join(like_conditions)})\")\n\n            where_clause = ' AND '.join(where_conditions)\n\n            query = f\"\"\"\n                SELECT * FROM articles \n                WHERE {where_clause}\n                ORDER BY date_published DESC, fetched_at DESC \n                LIMIT ?\n            \"\"\"\n            params.append(limit)\n\n            with sqlite3.connect(self.db_path) as conn:\n                conn.row_factory = sqlite3.Row\n                cursor = conn.cursor()\n                cursor.execute(query, params)\n                rows = cursor.fetchall()\n\n                articles = []\n                for row in rows:\n                    article = dict(row)\n                    # Parse dates\n                    article['date_published'] = self._parse_date(article['date_published'])\n                    article['date_modified'] = self._parse_date(article['date_modified'])  \n                    article['fetched_at'] = self._parse_date(article['fetched_at'])\n                    articles.append(article)\n\n                logger.debug(f\"Retrieved {len(articles)} articles (limit={limit}, source={source}, section={section})\")\n                return articles\n\n        except Exception as e:\n            logger.error(f\"Error getting recent articles: {e}\")\n            return []\n\n    def _parse_date(self, date_str):\n        \"\"\"Helper to parse date strings, returning None for invalid inputs.\"\"\"\n        if not date_str:\n            return None\n        try:\n            # Attempt to parse as ISO format first\n            return datetime.fromisoformat(date_str)\n        except ValueError:\n            try:\n                # Fallback for other potential formats if needed, though ISO is preferred\n                # Example: return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n                return None # Or handle other formats as necessary\n            except ValueError:\n                return None\n\n    def get_stats(self):\n        \"\"\"Get basic statistics about stored articles\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                # Total articles count\n                cursor = conn.execute('SELECT COUNT(*) FROM articles')\n                total_articles = cursor.fetchone()[0]\n\n                # Last update time\n                cursor = conn.execute('''\n                    SELECT MAX(fetched_at) FROM articles\n                ''')\n                last_update = cursor.fetchone()[0]\n\n                return {\n                    'total_articles': total_articles,\n                    'last_update': last_update\n                }\n\n        except Exception as e:\n            logger.error(f\"Error getting stats: {e}\")\n            return {\n                'total_articles': 0,\n                'last_update': None\n            }\n\n    def get_detailed_stats(self):\n        \"\"\"Get detailed statistics for admin interface\"\"\"\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                stats = {}\n\n                # Basic counts\n                cursor = conn.execute('SELECT COUNT(*) FROM articles')\n                stats['total_articles'] = cursor.fetchone()[0]\n\n                # Articles by date ranges\n                now = datetime.utcnow()\n                day_ago = now - timedelta(days=1)\n                week_ago = now - timedelta(days=7)\n\n                cursor = conn.execute('''\n                    SELECT COUNT(*) FROM articles \n                    WHERE fetched_at >= ?\n                ''', (day_ago.isoformat(),))\n                stats['articles_last_24h'] = cursor.fetchone()[0]\n\n                cursor = conn.execute('''\n                    SELECT COUNT(*) FROM articles \n                    WHERE fetched_at >= ?\n                ''', (week_ago.isoformat(),))\n                stats['articles_last_week'] = cursor.fetchone()[0]\n\n                # Last update\n                cursor = conn.execute('SELECT MAX(fetched_at) FROM articles')\n                stats['last_update'] = cursor.fetchone()[0]\n\n                # Articles with images\n                cursor = conn.execute('SELECT COUNT(*) FROM articles WHERE image IS NOT NULL AND image != \"\"')\n                stats['articles_with_images'] = cursor.fetchone()[0]\n\n                # Top authors\n                cursor = conn.execute('''\n                    SELECT author, COUNT(*) as count \n                    FROM articles \n                    WHERE author IS NOT NULL AND author != \"\"\n                    GROUP BY author \n                    ORDER BY count DESC \n                    LIMIT 10\n                ''')\n                stats['top_authors'] = [{'author': row[0], 'count': row[1]} for row in cursor.fetchall()]\n\n                return stats\n\n        except Exception as e:\n            logger.error(f\"Error getting detailed stats: {e}\")\n            return {}\n\n    def cleanup_old_articles(self, days_to_keep=30):\n        \"\"\"Remove articles older than specified days\"\"\"\n        try:\n            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)\n\n            with sqlite3.connect(self.db_path) as conn:\n                cursor = conn.execute('''\n                    DELETE FROM articles \n                    WHERE fetched_at < ?\n                ''', (cutoff_date.isoformat(),))\n\n                deleted_count = cursor.rowcount\n                conn.commit()\n\n                logger.info(f\"Cleaned up {deleted_count} old articles\")\n                return deleted_count\n\n        except Exception as e:\n            logger.error(f\"Error cleaning up old articles: {e}\")\n            return 0","size_bytes":14455},"app/utils.py":{"content":"import re\nimport logging\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dateutil import parser as date_parser\n\nlogger = logging.getLogger(__name__)\n\ndef get_user_agent():\n    \"\"\"Get a polite, identifiable User-Agent string\"\"\"\n    return \"Mozilla/5.0 (compatible; LanceFeedBot/1.0; +https://lance-feeds.repl.co/)\"\n\ndef normalize_date(date_string):\n    \"\"\"Normalize various date formats to datetime object with UTC timezone\"\"\"\n    if not date_string:\n        return None\n    \n    try:\n        if isinstance(date_string, datetime):\n            # If already a datetime, ensure it has timezone info\n            if date_string.tzinfo is None:\n                return date_string.replace(tzinfo=timezone.utc)\n            return date_string\n        \n        # Parse the date string\n        parsed_date = date_parser.parse(date_string)\n        \n        # Ensure timezone info is present\n        if parsed_date.tzinfo is None:\n            parsed_date = parsed_date.replace(tzinfo=timezone.utc)\n        \n        return parsed_date\n        \n    except (ValueError, TypeError) as e:\n        logger.warning(f\"Could not parse date '{date_string}': {e}\")\n        return None\n\ndef extract_mime_type(image_url):\n    \"\"\"Extract MIME type from image URL based on extension\"\"\"\n    if not image_url:\n        return 'image/jpeg'  # Default fallback\n    \n    # Extract file extension\n    parsed_url = urlparse(image_url)\n    path = parsed_url.path.lower()\n    \n    if path.endswith('.png'):\n        return 'image/png'\n    elif path.endswith('.jpg') or path.endswith('.jpeg'):\n        return 'image/jpeg'\n    elif path.endswith('.gif'):\n        return 'image/gif'\n    elif path.endswith('.webp'):\n        return 'image/webp'\n    elif path.endswith('.svg'):\n        return 'image/svg+xml'\n    else:\n        return 'image/jpeg'  # Default fallback\n\ndef validate_admin_key(provided_key, expected_key):\n    \"\"\"Validate admin key for protected endpoints\"\"\"\n    if not expected_key:\n        return False  # No admin key configured\n    \n    return provided_key == expected_key\n\ndef parse_query_filter(query_string):\n    \"\"\"Parse and validate query filter string\"\"\"\n    if not query_string:\n        return None\n    \n    # Basic sanitization - remove potentially dangerous characters\n    # Allow alphanumeric, spaces, and common punctuation\n    sanitized = re.sub(r'[^\\w\\s\\-\\|\\(\\)]+', '', query_string)\n    \n    # Limit length to prevent abuse\n    if len(sanitized) > 100:\n        sanitized = sanitized[:100]\n    \n    return sanitized.strip()\n\ndef format_rfc2822_date(dt):\n    \"\"\"Format datetime for RSS (RFC 2822)\"\"\"\n    if not dt:\n        return None\n    \n    if isinstance(dt, str):\n        dt = normalize_date(dt)\n    \n    if not dt:\n        return None\n    \n    # Format as RFC 2822 (e.g., \"Wed, 02 Oct 2024 15:00:00 +0000\")\n    return dt.strftime('%a, %d %b %Y %H:%M:%S +0000')\n\ndef format_iso8601_date(dt):\n    \"\"\"Format datetime for Atom (ISO 8601)\"\"\"\n    if not dt:\n        return None\n    \n    if isinstance(dt, str):\n        dt = normalize_date(dt)\n    \n    if not dt:\n        return None\n    \n    # Format as ISO 8601 (e.g., \"2024-10-02T15:00:00Z\")\n    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\ndef sanitize_html(text):\n    \"\"\"Basic HTML sanitization for feed content\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove potentially dangerous HTML tags\n    dangerous_tags = re.compile(r'<(script|style|iframe|object|embed)[^>]*>.*?</\\1>', re.IGNORECASE | re.DOTALL)\n    text = dangerous_tags.sub('', text)\n    \n    # Remove HTML comments\n    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n    \n    return text.strip()\n\ndef truncate_text(text, max_length=500):\n    \"\"\"Truncate text to specified length with ellipsis\"\"\"\n    if not text or len(text) <= max_length:\n        return text\n    \n    # Find last space before max_length to avoid cutting words\n    truncated = text[:max_length]\n    last_space = truncated.rfind(' ')\n    \n    if last_space > max_length * 0.8:  # Only use space if it's not too far back\n        truncated = truncated[:last_space]\n    \n    return truncated + '...'\n\ndef is_valid_url(url):\n    \"\"\"Check if URL is valid and from allowed domains\"\"\"\n    try:\n        parsed = urlparse(url)\n        \n        # Check if URL has scheme and netloc\n        if not parsed.scheme or not parsed.netloc:\n            return False\n        \n        # Check if it's from lance.com.br\n        if not parsed.netloc.endswith('lance.com.br'):\n            return False\n        \n        return True\n        \n    except Exception:\n        return False\n\ndef clean_text(text):\n    \"\"\"Clean text content for feed display\"\"\"\n    if not text:\n        return \"\"\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove common web artifacts\n    text = re.sub(r'\\n|\\r|\\t', ' ', text)\n    \n    # Trim and return\n    return text.strip()\n","size_bytes":4877},"static/style.css":{"content":"/* Custom styles for Lance Feed Generator */\n\nbody {\n    background: linear-gradient(135deg, #1a1a1a 0%, #2d2d2d 100%);\n    min-height: 100vh;\n}\n\n.card {\n    transition: transform 0.2s ease-in-out;\n}\n\n.card:hover {\n    transform: translateY(-2px);\n}\n\n.input-group .form-control {\n    font-family: 'Courier New', monospace;\n    font-size: 0.9rem;\n}\n\ncode {\n    background-color: var(--bs-gray-800);\n    color: var(--bs-cyan);\n    padding: 0.2rem 0.4rem;\n    border-radius: 0.25rem;\n    font-size: 0.875rem;\n}\n\n.table code {\n    background-color: var(--bs-gray-700);\n    color: var(--bs-warning);\n}\n\n.bg-secondary {\n    background-color: var(--bs-gray-800) !important;\n}\n\n.alert-warning {\n    background-color: rgba(255, 193, 7, 0.1);\n    border-color: var(--bs-warning);\n}\n\n.list-group-item:hover {\n    background-color: var(--bs-gray-700) !important;\n}\n\n/* Animation for copy button */\n.btn.btn-success {\n    transition: all 0.3s ease;\n}\n\n/* Responsive improvements */\n@media (max-width: 768px) {\n    .display-4 {\n        font-size: 2rem;\n    }\n    \n    .container {\n        padding: 0 15px;\n    }\n    \n    .input-group .form-control {\n        font-size: 0.8rem;\n    }\n}\n\n/* Loading animation */\n.spinner-border-sm {\n    width: 1rem;\n    height: 1rem;\n}\n\n/* Custom scrollbar for better dark theme consistency */\n::-webkit-scrollbar {\n    width: 8px;\n}\n\n::-webkit-scrollbar-track {\n    background: var(--bs-gray-900);\n}\n\n::-webkit-scrollbar-thumb {\n    background: var(--bs-gray-600);\n    border-radius: 4px;\n}\n\n::-webkit-scrollbar-thumb:hover {\n    background: var(--bs-gray-500);\n}\n","size_bytes":1583},"app/base_scraper.py":{"content":"\"\"\"\nBase scraper class that defines the interface for all news site scrapers\n\"\"\"\n\nimport logging\nimport requests\nimport json\nfrom urllib.robotparser import RobotFileParser\nfrom abc import ABC, abstractmethod\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom .utils import normalize_date, extract_mime_type, get_user_agent\n\nlogger = logging.getLogger(__name__)\n\ndef clean_image_url(url: str) -> str:\n    \"\"\"Clean image URL by removing CDN optimization parameters\"\"\"\n    if not url:\n        return url\n    \n    if \"/uploads/\" in url:\n        # Extract the host from the original URL\n        if url.startswith('http'):\n            from urllib.parse import urlparse\n            parsed = urlparse(url)\n            host = f\"{parsed.scheme}://{parsed.netloc}\"\n        else:\n            host = \"https://lncimg.lance.com.br\"\n        \n        # Keep the host + path from /uploads/\n        uploads_part = url.split(\"/uploads/\", 1)[-1]\n        return f\"{host}/uploads/{uploads_part}\"\n    \n    return url\n\nclass BaseScraper(ABC):\n    \"\"\"Abstract base class for all news site scrapers\"\"\"\n    \n    def __init__(self, store, request_delay=1.0):\n        self.store = store\n        self.request_delay = request_delay\n        self.session = requests.Session()\n        self.session.headers.update({'User-Agent': get_user_agent()})\n        self.robots_cache = {}\n    \n    @abstractmethod\n    def get_site_domain(self):\n        \"\"\"Return the main domain for this scraper\"\"\"\n        pass\n    \n    @abstractmethod\n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from a listing page HTML\"\"\"\n        pass\n    \n    @abstractmethod\n    def find_next_page_url(self, html, current_url):\n        \"\"\"Find the URL for the next page of articles\"\"\"\n        pass\n    \n    def apply_filters(self, article, filters):\n        \"\"\"Apply filters to an article and return True if article should be excluded\"\"\"\n        if not filters:\n            return False\n            \n        try:\n            # Check exclude_authors filter\n            if filters.get('exclude_authors'):\n                author = article.get('author', '').lower()\n                for excluded_author in filters['exclude_authors']:\n                    if excluded_author.lower() in author:\n                        logger.debug(f\"Excluding article by author: {author}\")\n                        return True\n            \n            # Check exclude_terms filter  \n            if filters.get('exclude_terms'):\n                title = article.get('title', '').lower()\n                description = article.get('description', '').lower()\n                \n                for term in filters['exclude_terms']:\n                    term_lower = term.lower()\n                    if term_lower in title or term_lower in description:\n                        logger.debug(f\"Excluding article containing term: {term}\")\n                        return True\n            \n            return False  # Article should be included\n            \n        except Exception as e:\n            logger.warning(f\"Error applying filters: {e}\")\n            return False  # Include article if filter evaluation fails\n    \n    def can_fetch(self, url):\n        \"\"\"Check if we can fetch the URL according to robots.txt\"\"\"\n        try:\n            from urllib.parse import urlparse\n            domain = urlparse(url).netloc\n            \n            if domain not in self.robots_cache:\n                robots_url = f\"https://{domain}/robots.txt\"\n                rp = RobotFileParser()\n                rp.set_url(robots_url)\n                try:\n                    rp.read()\n                    self.robots_cache[domain] = rp\n                except Exception as e:\n                    logger.warning(f\"Could not read robots.txt for {domain}: {e}\")\n                    # If we can't read robots.txt, assume we can fetch\n                    self.robots_cache[domain] = None\n            \n            rp = self.robots_cache[domain]\n            if rp is None:\n                return True\n            \n            user_agent = self.session.headers.get('User-Agent', get_user_agent())\n            can_fetch = rp.can_fetch(user_agent, url)\n            \n            logger.debug(f\"Robots check for {url} with UA '{user_agent[:50]}...': {can_fetch}\")\n            \n            # If robots.txt says no but we know the site allows scraping, proceed with caution\n            if not can_fetch:\n                logger.info(f\"Robots parser returned False for {self.get_site_domain()}, but robots.txt allows all. Proceeding.\")\n            \n            return True  # We generally allow scraping for news sites\n            \n        except Exception as e:\n            logger.warning(f\"Error checking robots.txt for {url}: {e}\")\n            return True\n    \n    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))\n    def _fetch_page(self, url):\n        \"\"\"Fetch a single page with retries (optimized for performance)\"\"\"\n        if not self.can_fetch(url):\n            logger.warning(f\"Robots.txt disallows fetching {url}\")\n            return None\n        \n        try:\n            response = self.session.get(url, timeout=15)  # Reduced timeout\n            response.raise_for_status()\n            return response.text\n        except Exception as e:\n            logger.error(f\"Error fetching {url}: {e}\")\n            raise\n    \n    def list_pages(self, start_url, max_pages=3, section=None):\n        \"\"\"Get article links from multiple pages starting from start_url\"\"\"\n        all_links = []\n        current_url = start_url\n        \n        for page_num in range(max_pages):\n            try:\n                logger.info(f\"Fetching page {page_num + 1}: {current_url}\")\n                html = self._fetch_page(current_url)\n                \n                if not html:\n                    logger.warning(f\"No content received from {current_url}\")\n                    break\n                \n                # Extract article links from current page\n                page_links = self.extract_article_links(html, current_url, section=section)\n                if not page_links:\n                    logger.warning(f\"No article links found on page {page_num + 1}\")\n                    break\n                \n                all_links.extend(page_links)\n                logger.info(f\"Found {len(page_links)} article links on page {page_num + 1}\")\n                \n                # Find next page URL\n                if page_num < max_pages - 1:\n                    next_url = self.find_next_page_url(html, current_url)\n                    if not next_url or next_url == current_url:\n                        logger.info(\"No more pages found\")\n                        break\n                    current_url = next_url\n                \n                # Delay between requests\n                if self.request_delay > 0:\n                    import time\n                    time.sleep(self.request_delay)\n                    \n            except Exception as e:\n                logger.error(f\"Error processing page {page_num + 1} ({current_url}): {e}\")\n                break\n        \n        logger.info(f\"Total article links collected: {len(all_links)}\")\n        return all_links\n    \n    def parse_json_ld(self, html):\n        \"\"\"Extract JSON-LD metadata from article page\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        \n        # Find all JSON-LD scripts\n        scripts = soup.find_all('script', {'type': 'application/ld+json'})\n        \n        for script in scripts:\n            try:\n                script_content = getattr(script, 'string', None) or ''\n                if not script_content or not isinstance(script_content, str):\n                    continue\n                data = json.loads(script_content)\n                \n                # Handle different JSON-LD structures\n                items = []\n                \n                if isinstance(data, dict):\n                    if '@graph' in data:\n                        items = data['@graph']\n                    else:\n                        items = [data]\n                elif isinstance(data, list):\n                    items = data\n                \n                # Look for Article/NewsArticle/BlogPosting\n                for item in items:\n                    if isinstance(item, dict):\n                        item_type = item.get('@type', '')\n                        if item_type in ['Article', 'NewsArticle', 'BlogPosting']:\n                            return self._extract_article_metadata(item)\n                            \n            except (json.JSONDecodeError, KeyError) as e:\n                logger.warning(f\"Error parsing JSON-LD: {e}\")\n                continue\n        \n        return None\n    \n    def _extract_article_metadata(self, json_ld):\n        \"\"\"Extract article metadata from JSON-LD object\"\"\"\n        metadata = {}\n        \n        # Title/headline\n        metadata['title'] = json_ld.get('headline', json_ld.get('name', ''))\n        \n        # Description\n        metadata['description'] = json_ld.get('description', '')\n        \n        # Image\n        image = json_ld.get('image')\n        if image:\n            raw_image_url = ''\n            if isinstance(image, str):\n                raw_image_url = image\n            elif isinstance(image, dict):\n                raw_image_url = image.get('url', '')\n            elif isinstance(image, list) and len(image) > 0:\n                if isinstance(image[0], str):\n                    raw_image_url = image[0]\n                elif isinstance(image[0], dict):\n                    raw_image_url = image[0].get('url', '')\n            \n            # Clean the image URL\n            metadata['image'] = clean_image_url(raw_image_url)\n        \n        # Dates\n        metadata['date_published'] = json_ld.get('datePublished', '')\n        metadata['date_modified'] = json_ld.get('dateModified', '')\n        \n        # Author\n        author = json_ld.get('author')\n        if author:\n            if isinstance(author, str):\n                metadata['author'] = author\n            elif isinstance(author, dict):\n                metadata['author'] = author.get('name', '')\n            elif isinstance(author, list) and len(author) > 0:\n                if isinstance(author[0], str):\n                    metadata['author'] = author[0]\n                elif isinstance(author[0], dict):\n                    metadata['author'] = author[0].get('name', '')\n        \n        return metadata\n    \n    def extract_fallback_metadata(self, html, url):\n        \"\"\"Extract fallback metadata from HTML meta tags\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        metadata = {}\n        \n        # Title fallback\n        title_tag = soup.find('title')\n        if title_tag:\n            metadata['title'] = title_tag.get_text().strip()\n        \n        # Description fallback\n        desc_meta = soup.find('meta', {'name': 'description'}) or \\\n                   soup.find('meta', {'property': 'og:description'})\n        if desc_meta and hasattr(desc_meta, 'get') and desc_meta.get('content'):\n            metadata['description'] = str(desc_meta.get('content', ''))\n        \n        # Image fallback\n        img_meta = soup.find('meta', {'property': 'og:image'})\n        if img_meta and hasattr(img_meta, 'get') and img_meta.get('content'):\n            raw_image_url = str(img_meta.get('content', ''))\n            metadata['image'] = clean_image_url(raw_image_url)\n        \n        return metadata\n    \n    def parse_article(self, url, source=None, section=None):\n        \"\"\"Parse individual article and extract metadata\"\"\"\n        try:\n            html = self._fetch_page(url)\n            if not html:\n                return None\n            \n            # Try JSON-LD first\n            metadata = self.parse_json_ld(html)\n            \n            # Fallback to HTML meta tags\n            if not metadata or not metadata.get('title'):\n                fallback = self.extract_fallback_metadata(html, url)\n                if not metadata:\n                    metadata = fallback\n                else:\n                    # Merge fallback data for missing fields\n                    for key, value in fallback.items():\n                        if not metadata.get(key):\n                            metadata[key] = value\n            \n            if not metadata:\n                return None\n            \n            # Normalize the article data\n            article = {\n                'url': url,\n                'title': metadata.get('title', '').strip(),\n                'description': metadata.get('description', '').strip(),\n                'image': clean_image_url(metadata.get('image', '').strip()),\n                'author': metadata.get('author', '').strip(),\n                'date_published': normalize_date(metadata.get('date_published')),\n                'date_modified': normalize_date(metadata.get('date_modified')),\n                'fetched_at': datetime.utcnow(),\n                'source': source or 'unknown',\n                'section': section or 'general',\n                'site': self.get_site_domain()\n            }\n            \n            # Skip articles without title\n            if not article['title']:\n                logger.warning(f\"Skipping article without title: {url}\")\n                return None\n            \n            return article\n            \n        except Exception as e:\n            logger.error(f\"Error parsing article {url}: {e}\")\n            return None\n    \n    def scrape_and_store(self, start_urls, max_pages=3, source=None, section=None, filters=None):\n        \"\"\"Main scraping method that collects and stores articles\"\"\"\n        # Handle both single URL and list of URLs\n        if isinstance(start_urls, str):\n            start_urls = [start_urls]\n        \n        logger.info(f\"Starting scrape from {len(start_urls)} source(s): {start_urls}\")\n        \n        # Get article links from all listing pages\n        all_article_urls = []\n        for start_url in start_urls:\n            logger.info(f\"Scraping from: {start_url}\")\n            article_urls = self.list_pages(start_url, max_pages)\n            all_article_urls.extend(article_urls)\n        \n        # Remove duplicates while preserving order\n        seen = set()\n        article_urls = []\n        for url in all_article_urls:\n            if url not in seen:\n                seen.add(url)\n                article_urls.append(url)\n        \n        if not article_urls:\n            logger.warning(\"No article URLs found\")\n            return []\n        \n        # Process each article\n        new_articles = []\n        for i, url in enumerate(article_urls):\n            try:\n                logger.info(f\"Processing article {i+1}/{len(article_urls)}: {url}\")\n                \n                # Check if we already have this article\n                if self.store.has_article(url):\n                    logger.debug(f\"Article already exists: {url}\")\n                    continue\n                \n                # Parse article\n                article = self.parse_article(url, source=source, section=section)\n                if not article:\n                    logger.warning(f\"Could not parse article: {url}\")\n                    continue\n                \n                # Apply filters\n                if filters and self._should_filter_article(article, filters):\n                    logger.info(f\"Article filtered out: {url}\")\n                    continue\n                \n                # Store article\n                if self.store.upsert_article(article):\n                    new_articles.append(article)\n                    logger.info(f\"Stored article: {article['title']}\")\n                \n                # Delay between requests\n                if self.request_delay > 0:\n                    import time\n                    time.sleep(self.request_delay)\n                    \n            except Exception as e:\n                logger.error(f\"Error processing article {url}: {e}\")\n                continue\n        \n        logger.info(f\"Scraping completed. New articles: {len(new_articles)}\")\n        return new_articles\n    \n    def _should_filter_article(self, article, filters):\n        \"\"\"Check if article should be filtered out based on filter rules\"\"\"\n        if not filters:\n            return False\n        \n        # Filter by excluded authors\n        exclude_authors = filters.get('exclude_authors', [])\n        if exclude_authors and article.get('author'):\n            author = article['author'].lower()\n            for excluded in exclude_authors:\n                if excluded.lower() in author:\n                    return True\n        \n        return False","size_bytes":16730},"app/folha_scraper.py":{"content":"\"\"\"\nFolha de S.Paulo-specific scraper implementation\n\"\"\"\n\nimport logging\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\nfrom .base_scraper import BaseScraper\n\nlogger = logging.getLogger(__name__)\n\nclass FolhaScraper(BaseScraper):\n    \"\"\"Scraper specifically designed for Folha de S.Paulo news sites\"\"\"\n    \n    def get_site_domain(self):\n        \"\"\"Return the main domain for Folha\"\"\"\n        return \"folha.uol.com.br\"\n    \n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from Folha listing pages\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        links = []\n        \n        # Folha uses specific structures for different sections\n        selectors = [\n            # Main article links\n            'a[href*=\"/poder/\"]',\n            'a[href*=\"/mercado/\"]',\n            'a[href*=\"/mundo/\"]',\n            'a[href*=\"/politica/\"]',\n            'a[href*=\"/economia/\"]',\n            # Generic news links\n            'h2 a[href]',\n            'h3 a[href]',\n            'h4 a[href]',\n            '.c-headline a[href]',\n            '.c-main-headline a[href]',\n            # Card and list layouts\n            '.c-news-item a[href]',\n            '.u-list-unstyled a[href]',\n            '.c-feed__content a[href]',\n            # Time-based listings\n            '.c-latest-news a[href]'\n        ]\n        \n        found_links = set()\n        \n        for selector in selectors:\n            try:\n                elements = soup.select(selector)\n                for element in elements:\n                    href = element.get('href')\n                    if not href:\n                        continue\n                    \n                    # Convert relative URLs to absolute\n                    if href.startswith('/'):\n                        full_url = urljoin(base_url, href)\n                    elif href.startswith('http'):\n                        full_url = href\n                    else:\n                        continue\n                    \n                    # Filter for Folha news articles\n                    if self._is_valid_folha_article_url(full_url):\n                        found_links.add(full_url)\n                        \n            except Exception as e:\n                logger.warning(f\"Error with selector {selector}: {e}\")\n                continue\n        \n        links = list(found_links)\n        logger.info(f\"Found {len(links)} article links on Folha page\")\n        return links\n    \n    def _is_valid_folha_article_url(self, url):\n        \"\"\"Check if URL is a valid Folha article\"\"\"\n        try:\n            parsed = urlparse(url)\n            \n            # Must be Folha domain\n            if 'folha.uol.com.br' not in parsed.netloc:\n                return False\n            \n            # Should contain news indicators\n            path = parsed.path.lower()\n            \n            # Valid patterns for Folha articles\n            valid_patterns = [\n                '/poder/',\n                '/mercado/',\n                '/mundo/',\n                '/politica/',\n                '/economia/',\n                '/cotidiano/',\n                '/esporte/',\n                '/ilustrada/'\n            ]\n            \n            if any(pattern in path for pattern in valid_patterns):\n                # Exclude non-article pages\n                exclude_patterns = [\n                    '/busca/',\n                    '/arquivo/',\n                    '/rss',\n                    '/feed',\n                    '/newsletter',\n                    '.rss',\n                    '.xml',\n                    '/blogs/',\n                    '/colunistas/'\n                ]\n                \n                if any(pattern in path for pattern in exclude_patterns):\n                    return False\n                \n                # Folha articles usually have dates in URL or are in news sections\n                if (any(pattern in path for pattern in valid_patterns) and \n                    (len(path.split('/')) >= 3)):  # Proper article structure\n                    return True\n            \n            return False\n            \n        except Exception as e:\n            logger.warning(f\"Error validating Folha URL {url}: {e}\")\n            return False\n    \n    def find_next_page_url(self, html, current_url):\n        \"\"\"Find the URL for the next page of Folha articles\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        \n        # Folha pagination selectors\n        next_selectors = [\n            'a[rel=\"next\"]',\n            '.c-pagination a[href*=\"page=\"]',\n            '.pagination a[href*=\"pagina=\"]',\n            'a[href*=\"proxima\"]',\n            'a[href*=\"next\"]',\n            '.next a[href]',\n            '.proximo a[href]',\n            '.c-pagination__next[href]'\n        ]\n        \n        for selector in next_selectors:\n            try:\n                next_link = soup.select_one(selector)\n                if next_link and next_link.get('href'):\n                    href = next_link.get('href')\n                    \n                    # Convert relative URLs to absolute\n                    if href.startswith('/'):\n                        return urljoin(current_url, href)\n                    elif href.startswith('http'):\n                        return href\n                        \n            except Exception as e:\n                logger.warning(f\"Error finding next page with selector {selector}: {e}\")\n                continue\n        \n        # Try to find \"Load More\" or infinite scroll patterns\n        try:\n            load_more_selectors = [\n                'button[data-load-more]',\n                '.load-more[href]',\n                '.c-load-more[href]',\n                '[data-next-page]'\n            ]\n            \n            for selector in load_more_selectors:\n                element = soup.select_one(selector)\n                if element:\n                    # For Folha, this might be AJAX-based, so we'll skip for now\n                    logger.info(\"Found load-more button, but skipping AJAX pagination\")\n                    break\n                    \n        except Exception as e:\n            logger.warning(f\"Error finding load-more elements: {e}\")\n        \n        return None\n    \n    def parse_article(self, url, source=None, section=None):\n        \"\"\"Parse Folha article with specific handling\"\"\"\n        article = super().parse_article(url, source=source, section=section)\n        \n        if article:\n            # Additional Folha-specific processing\n            article = self._enhance_folha_metadata(article, url)\n        \n        return article\n    \n    def _enhance_folha_metadata(self, article, url):\n        \"\"\"Add Folha-specific metadata enhancements\"\"\"\n        try:\n            # Folha-specific author cleaning\n            if article.get('author'):\n                author = article['author']\n                # Remove common Folha suffixes\n                author = author.replace(' - Folha', '')\n                author = author.replace('Folha ', '')\n                author = author.replace(' da Folha', '')\n                article['author'] = author.strip()\n            \n            # Extract section from URL for better categorization\n            parsed = urlparse(url)\n            path_parts = parsed.path.strip('/').split('/')\n            \n            if len(path_parts) > 1:\n                url_section = path_parts[0].lower()\n                if url_section in ['poder', 'mercado', 'mundo', 'cotidiano', 'esporte']:\n                    # Map Folha sections to standard names\n                    section_mapping = {\n                        'poder': 'politica',\n                        'mercado': 'economia',\n                        'mundo': 'mundo',\n                        'cotidiano': 'geral',\n                        'esporte': 'esporte'\n                    }\n                    article['section'] = section_mapping.get(url_section, url_section)\n            \n            # Clean up description from Folha-specific patterns\n            if article.get('description'):\n                desc = article['description']\n                # Remove \"Leia mais\" and similar patterns\n                desc = desc.replace('Leia mais...', '').replace('Continuar lendo', '')\n                desc = desc.replace('Saiba mais', '').strip()\n                article['description'] = desc\n            \n            return article\n            \n        except Exception as e:\n            logger.warning(f\"Error enhancing Folha metadata for {url}: {e}\")\n            return article","size_bytes":8508},"app/scraper_factory.py":{"content":"\"\"\"\nFactory for creating scrapers for different news sources\n\"\"\"\n\nimport logging\nfrom .lance_scraper import LanceScraper\nfrom .uol_scraper import UolScraper  \nfrom .folha_scraper import FolhaScraper\nfrom .gazeta_scraper import GazetaScraper\nfrom .sources_config import SOURCES_CONFIG\n\nlogger = logging.getLogger(__name__)\n\nclass ScraperFactory:\n    \"\"\"Factory for creating appropriate scrapers based on source\"\"\"\n    \n    _scrapers = {}\n    \n    @classmethod\n    def get_scraper(cls, source, store, request_delay=1.0):\n        \"\"\"Get or create a scraper for the specified source\"\"\"\n        if source in cls._scrapers:\n            return cls._scrapers[source]\n        \n        source_config = SOURCES_CONFIG.get(source)\n        if not source_config:\n            raise ValueError(f\"Unknown source: {source}\")\n        \n        scraper_class_name = source_config.get('scraper_class')\n        if not scraper_class_name:\n            raise ValueError(f\"No scraper class defined for source: {source}\")\n        \n        # Map scraper class names to actual classes\n        scraper_classes = {\n            'LanceScraper': LanceScraper,\n            'UolScraper': UolScraper,\n            'FolhaScraper': FolhaScraper,\n            'GazetaScraper': GazetaScraper\n        }\n        \n        scraper_class = scraper_classes.get(scraper_class_name)\n        if not scraper_class:\n            raise ValueError(f\"Unknown scraper class: {scraper_class_name}\")\n        \n        # Create and cache scraper instance\n        scraper = scraper_class(store, request_delay=request_delay)\n        cls._scrapers[source] = scraper\n        \n        logger.info(f\"Created {scraper_class_name} for source '{source}'\")\n        return scraper\n    \n    @classmethod\n    def get_all_scrapers(cls, store, request_delay=1.0):\n        \"\"\"Get scrapers for all configured sources\"\"\"\n        scrapers = {}\n        for source in SOURCES_CONFIG.keys():\n            try:\n                scrapers[source] = cls.get_scraper(source, store, request_delay)\n            except Exception as e:\n                logger.error(f\"Failed to create scraper for {source}: {e}\")\n        return scrapers\n    \n    @classmethod\n    def clear_cache(cls):\n        \"\"\"Clear cached scrapers (useful for testing)\"\"\"\n        cls._scrapers.clear()\n    \n    @classmethod\n    def scrape_source_section(cls, source, section, store, max_pages=2, max_articles=20, request_delay=0.3):\n        \"\"\"Scrape a specific source and section with performance optimizations\"\"\"\n        try:\n            scraper = cls.get_scraper(source, store, request_delay)\n            source_config = SOURCES_CONFIG[source]\n            section_config = source_config['sections'][section]\n            \n            start_urls = section_config.get('start_urls', [])\n            if not start_urls:\n                logger.warning(f\"No start URLs configured for {source}/{section}\")\n                return []\n            \n            # For now, scrape the first URL\n            # TODO: Implement multi-URL scraping\n            url = start_urls[0]\n            \n            logger.info(f\"Scraping {source}/{section} from {url} (max_articles: {max_articles})\")\n            \n            # Use optimized scraping with article limits to prevent timeouts\n            all_article_urls = scraper.list_pages(url, max_pages, section=section)\n            \n            # Limit articles to prevent worker timeouts\n            limited_urls = all_article_urls[:max_articles] if len(all_article_urls) > max_articles else all_article_urls\n            logger.info(f\"Processing {len(limited_urls)} articles (found {len(all_article_urls)})\")\n            \n            new_articles = []\n            filters = section_config.get('filters', {})\n            \n            for i, article_url in enumerate(limited_urls, 1):\n                try:\n                    logger.info(f\"Processing article {i}/{len(limited_urls)}: {article_url}\")\n                    \n                    # Parse article\n                    article = scraper.parse_article(article_url, source=source, section=section)\n                    if not article:\n                        continue\n                    \n                    # Apply filters\n                    if filters and scraper.apply_filters(article, filters):\n                        logger.info(f\"Article filtered out: {article_url}\")\n                        continue\n                    \n                    # Store article\n                    if scraper.store.upsert_article(article):\n                        new_articles.append(article)\n                        logger.info(f\"Stored article: {article['title']}\")\n                    \n                    # Reduced delay for performance\n                    if i < len(limited_urls):\n                        import time\n                        time.sleep(request_delay)\n                        \n                except Exception as e:\n                    logger.error(f\"Error processing article {article_url}: {e}\")\n                    continue\n            \n            return new_articles\n            \n        except Exception as e:\n            logger.error(f\"Failed to scrape {source}/{section}: {e}\")\n            return []","size_bytes":5170},"app/sources_config.py":{"content":"\"\"\"\nConfiguration for multiple news sources and their sections\n\"\"\"\n\nSOURCES_CONFIG = {\n    'gazeta': {\n        'name': 'Gazeta Esportiva',\n        'base_url': 'https://www.gazetaesportiva.com',\n        'scraper_class': 'GazetaScraper',\n        'language': 'pt-BR',\n        'sections': {\n            'todas-noticias': {\n                'name': 'Todas as Not√≠cias',\n                'description': 'Todas as not√≠cias da Gazeta Esportiva',\n                'start_urls': ['https://www.gazetaesportiva.com/todas-as-noticias/'],\n                'filters': {}\n            },\n            'futebol': {\n                'name': 'Futebol',\n                'description': 'Not√≠cias de futebol da Gazeta Esportiva',\n                'start_urls': ['https://www.gazetaesportiva.com/futebol/'],\n                'filters': {}\n            },\n            'brasileirao': {\n                'name': 'Brasileir√£o',\n                'description': 'Not√≠cias do Campeonato Brasileiro',\n                'start_urls': ['https://www.gazetaesportiva.com/brasileirao/'],\n                'filters': {}\n            }\n        }\n    },\n    'lance': {\n        'name': 'LANCE!',\n        'base_url': 'https://www.lance.com.br',\n        'language': 'pt-BR',\n        'scraper_class': 'LanceScraper',\n        'sections': {\n            'futebol': {\n                'name': 'Futebol',\n                'start_urls': [\n                    'https://www.lance.com.br/mais-noticias',\n                    'https://www.lance.com.br/brasileirao',\n                    'https://www.lance.com.br/futebol-nacional',\n                    'https://www.lance.com.br/futebol-internacional'\n                ],\n                'description': 'Not√≠cias de futebol e esportes',\n                'filters': {}\n            }\n        }\n    },\n    'uol': {\n        'name': 'UOL',\n        'base_url': 'https://www.uol.com.br',\n        'language': 'pt-BR',\n        'scraper_class': 'UolScraper',\n        'sections': {\n            'economia': {\n                'name': 'Economia',\n                'start_urls': ['https://www.uol.com.br/'],\n                'description': 'Not√≠cias de economia do UOL',\n                'filters': {}\n            },\n            'politica': {\n                'name': 'Pol√≠tica',\n                'start_urls': ['https://noticias.uol.com.br/politica/'],\n                'description': 'Not√≠cias de pol√≠tica do UOL',\n                'filters': {}\n            },\n            'mundo': {\n                'name': 'Mundo',\n                'start_urls': ['https://noticias.uol.com.br/internacional/'],\n                'description': 'Not√≠cias internacionais do UOL',\n                'filters': {}\n            },\n            'futebol': {\n                'name': 'Futebol',\n                'start_urls': ['https://www.uol.com.br/esporte/futebol/ultimas/'],\n                'description': 'Not√≠cias de futebol do UOL',\n                'filters': {\n                    'exclude_authors': ['Gazeta Esportiva', 'gazeta-esportiva']\n                }\n            }\n        }\n    },\n    'folha': {\n        'name': 'Folha de S.Paulo',\n        'base_url': 'https://www1.folha.uol.com.br',\n        'language': 'pt-BR',\n        'scraper_class': 'FolhaScraper',\n        'sections': {\n            'politica': {\n                'name': 'Pol√≠tica',\n                'start_urls': ['https://www1.folha.uol.com.br/poder/'],\n                'description': 'Not√≠cias de pol√≠tica da Folha',\n                'filters': {}\n            },\n            'economia': {\n                'name': 'Economia',\n                'start_urls': ['https://www1.folha.uol.com.br/mercado/'],\n                'description': 'Not√≠cias de economia da Folha',\n                'filters': {}\n            },\n            'mundo': {\n                'name': 'Mundo',\n                'start_urls': ['https://www1.folha.uol.com.br/mundo/'],\n                'description': 'Not√≠cias internacionais da Folha',\n                'filters': {}\n            }\n        }\n    }\n}\n\n# Alias for easier imports\nSOURCES = SOURCES_CONFIG\n\ndef get_source_config(source):\n    \"\"\"Get configuration for a specific source\"\"\"\n    return SOURCES_CONFIG.get(source)\n\ndef get_section_config(source, section):\n    \"\"\"Get configuration for a specific source section\"\"\"\n    source_config = get_source_config(source)\n    if not source_config:\n        return None\n    return source_config.get('sections', {}).get(section)\n\ndef get_all_sources():\n    \"\"\"Get list of all available sources\"\"\"\n    return list(SOURCES_CONFIG.keys())\n\ndef get_source_sections(source):\n    \"\"\"Get list of all sections for a source\"\"\"\n    source_config = get_source_config(source)\n    if not source_config:\n        return []\n    return list(source_config.get('sections', {}).keys())\n\ndef validate_source_section(source, section):\n    \"\"\"Validate if source and section combination exists\"\"\"\n    return get_section_config(source, section) is not None","size_bytes":4904},"app/uol_scraper.py":{"content":"\"\"\"\nUOL-specific scraper implementation\n\"\"\"\n\nimport logging\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\nimport requests\nfrom tenacity import retry, wait_exponential, stop_after_attempt\nfrom .base_scraper import BaseScraper\n\nlogger = logging.getLogger(__name__)\n\nclass UolScraper(BaseScraper):\n    \"\"\"Scraper specifically designed for UOL news sites\"\"\"\n    \n    def __init__(self, store, request_delay=1.0):\n        super().__init__(store, request_delay)\n        # Use standard browser User-Agent for UOL to avoid blocking\n        self.browser_headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n            'Referer': 'https://www.uol.com.br/',\n            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n        }\n        self.session.headers.update(self.browser_headers)\n        \n    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))\n    def _fetch_page(self, url):\n        \"\"\"Override to use enhanced browser headers for UOL requests\"\"\"\n        if not self.can_fetch(url):\n            logger.warning(f\"Robots.txt disallows fetching {url}\")\n            return None\n        \n        # Enhanced browser headers for anti-bot protection\n        enhanced_headers = {\n            **self.browser_headers,\n            'sec-ch-ua': '\"Chromium\";v=\"91\", \" Not A;Brand\";v=\"99\"',\n            'sec-ch-ua-mobile': '?0',\n            'sec-ch-ua-platform': '\"Windows\"',\n            'sec-fetch-dest': 'document',\n            'sec-fetch-mode': 'navigate',\n            'sec-fetch-site': 'same-origin',\n            'upgrade-insecure-requests': '1',\n            'accept-encoding': 'gzip, deflate, br',\n            'cache-control': 'max-age=0'\n        }\n        \n        try:\n            response = self.session.get(url, headers=enhanced_headers, timeout=15)\n            response.raise_for_status()\n            return response.text\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 403:\n                # Try AMP version as fallback\n                if '/amp' not in url and '?amp' not in url:\n                    try:\n                        amp_url = url.rstrip('/') + '/amp'\n                        logger.info(f\"Trying AMP version: {amp_url}\")\n                        response = self.session.get(amp_url, headers=enhanced_headers, timeout=15)\n                        response.raise_for_status()\n                        return response.text\n                    except:\n                        pass\n            logger.error(f\"Error fetching {url}: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error fetching {url}: {e}\")\n            raise\n    \n    def get_site_domain(self):\n        \"\"\"Return the main domain for UOL\"\"\"\n        return \"uol.com.br\"\n    \n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from UOL listing pages with section filtering\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        links = []\n        \n        # Section-specific selectors for better filtering\n        section_selectors = {\n            'futebol': [\n                'a[href*=\"/esporte/futebol/\"]',\n                'a[href*=\"/futebol/\"]'\n            ],\n            'economia': [\n                'a[href*=\"economia.uol.com.br\"]',\n                'a[href*=\"/play/videos/economia/\"]',\n                'a[href*=\"/economia/\"]',\n                'a[href*=\"/mercado/\"]'\n            ],\n            'politica': [\n                'a[href*=\"/politica/\"]',\n                'a[href*=\"/poder/\"]'\n            ],\n            'mundo': [\n                'a[href*=\"/internacional/\"]',\n                'a[href*=\"/mundo/\"]'\n            ]\n        }\n        \n        # Use section-specific selectors if available, otherwise generic\n        if section and section in section_selectors:\n            selectors = section_selectors[section]\n        else:\n            # Fallback to generic selectors\n            selectors = [\n                'a[href*=\"/noticias/\"]',\n                'a[href*=\"/economia/\"]',\n                'a[href*=\"/politica/\"]', \n                'a[href*=\"/internacional/\"]',\n                'a[href*=\"/esporte/futebol/\"]'\n            ]\n        \n        # Add generic layout selectors\n        selectors.extend([\n            'h1 a[href]',\n            'h2 a[href]',\n            'h3 a[href]',\n            'h4 a[href]',\n            '.manchete a[href]',\n            '.chamada a[href]',\n            '.card a[href]',\n            '.item a[href]',\n            '.news-item a[href]',\n            '.lista-noticias a[href]',\n            '.conteudo-destaque a[href]',\n            '.box-noticia a[href]',\n            '.destaque a[href]',\n            '.thumb-materia a[href]',\n            'article a[href]',\n            '.artigo a[href]'\n        ])\n        \n        found_links = set()\n        \n        for selector in selectors:\n            try:\n                elements = soup.select(selector)\n                for element in elements:\n                    href = element.get('href')\n                    if not href:\n                        continue\n                    \n                    # Convert relative URLs to absolute\n                    if href.startswith('/'):\n                        full_url = urljoin(base_url, href)\n                    elif href.startswith('http'):\n                        full_url = href\n                    else:\n                        continue\n                    \n                    # Filter for UOL news articles with section filtering\n                    if self._is_valid_uol_article_url(full_url, section):\n                        found_links.add(full_url)\n                        \n            except Exception as e:\n                logger.warning(f\"Error with selector {selector}: {e}\")\n                continue\n        \n        links = list(found_links)\n        logger.info(f\"Found {len(links)} article links on UOL page\")\n        return links\n    \n    def _is_valid_uol_article_url(self, url, section=None):\n        \"\"\"Check if URL is a valid UOL article with optional section filtering\"\"\"\n        try:\n            parsed = urlparse(url)\n            \n            # Must be UOL domain (not Folha)\n            valid_uol_domains = [\n                'www.uol.com.br',\n                'economia.uol.com.br', \n                'noticias.uol.com.br',\n                'esporte.uol.com.br',\n                'play.uol.com.br'\n            ]\n            \n            if not any(domain in parsed.netloc for domain in valid_uol_domains):\n                return False\n                \n            # Explicitly exclude Folha domains\n            if 'folha.uol.com.br' in parsed.netloc:\n                return False\n            \n            # Should contain news indicators\n            path = parsed.path.lower()\n            \n            # Section-specific patterns for better filtering\n            section_patterns = {\n                'futebol': ['/esporte/futebol/', '/futebol/'],\n                'economia': ['/economia/', '/mercado/'],\n                'politica': ['/politica/', '/poder/'],\n                'mundo': ['/internacional/', '/mundo/']\n            }\n            \n            # Special handling for economia section - accept economia subdomain but avoid listing pages\n            if section == 'economia' and 'economia.uol.com.br' in parsed.netloc:\n                # Exclude home, listing, and non-article pages\n                exclude_patterns = [\n                    '/busca/', '/arquivo/', '/rss', '/feed', '/ultimas/', '.rss', '.xml',\n                    '/$',  # Home page\n                    '/temas/', '/podcast/', '/guia-de-', '/cotacoes/', '/empregos-e-carreiras/',\n                    '/dinheiro-e-renda/', '/empresas-e-negocios/', '/imposto-de-renda/'\n                ]\n                # Only accept URLs that look like actual articles (with dates or specific patterns)\n                if (any(pattern in path for pattern in exclude_patterns) or \n                    path.count('/') < 2 or  # Too shallow, likely listing page\n                    not any(char.isdigit() for char in path)):  # No dates/numbers, likely listing\n                    return False\n                return True\n            \n            # Also accept UOL Play economia videos\n            if section == 'economia' and 'play.uol.com.br' in parsed.netloc and '/economia/' in path:\n                return True\n            \n            # If section is specified, only allow articles from that section\n            if section and section in section_patterns:\n                valid_patterns = section_patterns[section]\n            else:\n                # Fallback to all patterns\n                valid_patterns = [\n                    '/noticias/',\n                    '/economia/',\n                    '/politica/',\n                    '/internacional/',\n                    '/esporte/futebol/',\n                    '/mercado/',\n                    '/poder/',\n                    '/mundo/'\n                ]\n            \n            if any(pattern in path for pattern in valid_patterns):\n                # Exclude non-article pages\n                exclude_patterns = [\n                    '/busca/',\n                    '/arquivo/',\n                    '/rss',\n                    '/feed',\n                    '/ultimas/',\n                    '.rss',\n                    '.xml',\n                    '/times/',  # Team pages, not articles\n                    '/tabela',  # League tables\n                    '/classificacao'  # Standings\n                ]\n                \n                if any(pattern in path for pattern in exclude_patterns):\n                    return False\n                \n                # Additional section-specific exclusions\n                if section == 'futebol':\n                    # Exclude non-football URLs when scraping football\n                    non_football_patterns = ['/politica/', '/economia/', '/internacional/', '/poder/', '/mercado/', '/mundo/']\n                    if any(pattern in path for pattern in non_football_patterns):\n                        return False\n                elif section == 'economia':\n                    # Exclude non-economy URLs when scraping economy\n                    non_economy_patterns = ['/politica/', '/futebol/', '/internacional/', '/poder/', '/mundo/']\n                    if any(pattern in path for pattern in non_economy_patterns):\n                        return False\n                \n                return True\n            \n            return False\n            \n        except Exception as e:\n            logger.warning(f\"Error validating UOL URL {url}: {e}\")\n            return False\n    \n    def find_next_page_url(self, html, current_url):\n        \"\"\"Find the URL for the next page of UOL articles\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        \n        # UOL pagination selectors\n        next_selectors = [\n            'a[rel=\"next\"]',\n            '.pagination a[href*=\"pagina=\"]',\n            '.paginacao a[href*=\"pagina=\"]',\n            'a[href*=\"proxima\"]',\n            'a[href*=\"next\"]',\n            '.next a[href]',\n            '.proximo a[href]'\n        ]\n        \n        for selector in next_selectors:\n            try:\n                next_link = soup.select_one(selector)\n                if next_link and next_link.get('href'):\n                    href = next_link.get('href')\n                    \n                    # Convert relative URLs to absolute\n                    if href.startswith('/'):\n                        return urljoin(current_url, href)\n                    elif href.startswith('http'):\n                        return href\n                        \n            except Exception as e:\n                logger.warning(f\"Error finding next page with selector {selector}: {e}\")\n                continue\n        \n        # Try to find pagination by page numbers\n        try:\n            # Look for numbered pagination\n            page_links = soup.select('a[href*=\"pagina=\"], a[href*=\"page=\"]')\n            if page_links:\n                # Get current page number from URL\n                from urllib.parse import parse_qs, urlparse\n                parsed_current = urlparse(current_url)\n                current_params = parse_qs(parsed_current.query)\n                current_page = int(current_params.get('pagina', current_params.get('page', ['1']))[0])\n                \n                # Look for next page\n                next_page = current_page + 1\n                for link in page_links:\n                    href = link.get('href', '')\n                    if f'pagina={next_page}' in href or f'page={next_page}' in href:\n                        if href.startswith('/'):\n                            return urljoin(current_url, href)\n                        elif href.startswith('http'):\n                            return href\n                            \n        except Exception as e:\n            logger.warning(f\"Error finding next page by number: {e}\")\n        \n        return None\n    \n    def parse_article(self, url, source=None, section=None):\n        \"\"\"Parse UOL article with specific handling\"\"\"\n        article = super().parse_article(url, source=source, section=section)\n        \n        if article:\n            # Additional UOL-specific processing\n            article = self._enhance_uol_metadata(article, url)\n        \n        return article\n    \n    def _enhance_uol_metadata(self, article, url):\n        \"\"\"Add UOL-specific metadata enhancements\"\"\"\n        try:\n            # UOL-specific author cleaning\n            if article.get('author'):\n                author = article['author']\n                # Remove common UOL suffixes\n                author = author.replace(' - UOL', '')\n                author = author.replace('UOL ', '')\n                article['author'] = author.strip()\n            \n            # Extract section from URL for better categorization\n            parsed = urlparse(url)\n            path_parts = parsed.path.strip('/').split('/')\n            \n            if len(path_parts) > 1:\n                url_section = path_parts[0].lower()\n                if url_section in ['economia', 'politica', 'internacional', 'esporte']:\n                    article['section'] = url_section\n            \n            return article\n            \n        except Exception as e:\n            logger.warning(f\"Error enhancing UOL metadata for {url}: {e}\")\n            return article","size_bytes":14581},"app/gazeta_scraper.py":{"content":"\n\"\"\"\nGazeta Esportiva scraper compatible with the multi-source system\n\"\"\"\n\nimport logging\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom .base_scraper import BaseScraper\nfrom .utils import get_user_agent\n\nlogger = logging.getLogger(__name__)\n\nclass GazetaScraper(BaseScraper):\n    \"\"\"Scraper specifically designed for Gazeta Esportiva\"\"\"\n    \n    def get_site_domain(self):\n        \"\"\"Return the main domain for this scraper\"\"\"\n        return 'gazetaesportiva.com'\n    \n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from listing page HTML\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        links = []\n\n        # Find all links that point to articles\n        for link in soup.find_all('a', href=True):\n            href = link.get('href', '')\n\n            # Convert relative URLs to absolute\n            if href and isinstance(href, str) and href.startswith('/'):\n                href = urljoin(base_url, href)\n\n            # Filter for Gazeta articles (excluding external links and non-article pages)\n            if (href and isinstance(href, str) and \n                href.startswith('https://www.gazetaesportiva.com/') and \n                not href.endswith('/') and\n                '/' in href.split('gazetaesportiva.com/')[-1] and\n                not any(x in href for x in ['#', '?page=', '/categoria/', '/tag/', '/autor/'])):\n                links.append(href)\n\n        # Remove duplicates while preserving order\n        seen = set()\n        unique_links = []\n        for link in links:\n            if link not in seen:\n                seen.add(link)\n                unique_links.append(link)\n\n        logger.info(f\"Extracted {len(unique_links)} article links from {base_url}\")\n        return unique_links\n\n    def find_next_page_url(self, html, current_url):\n        \"\"\"Find the URL for the next page of articles\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n\n        # Look for pagination links\n        next_link = soup.find('a', {'class': 'next'}) or \\\n                   soup.find('a', string=lambda text: text and 'pr√≥xima' in text.lower()) or \\\n                   soup.find('a', string=lambda text: text and 'next' in text.lower())\n        \n        if next_link and hasattr(next_link, 'get') and next_link.get('href'):\n            next_url = next_link.get('href', '')\n            if next_url and isinstance(next_url, str):\n                if next_url.startswith('/'):\n                    next_url = urljoin(current_url, next_url)\n                return next_url\n\n        # Look for numbered pagination\n        page_links = soup.find_all('a', href=True)\n        current_page = 1\n        \n        # Try to extract current page from URL\n        if '?page=' in current_url:\n            try:\n                current_page = int(current_url.split('?page=')[-1].split('&')[0])\n            except:\n                pass\n        \n        # Look for next page number\n        for link in page_links:\n            href = link.get('href', '')\n            if f'?page={current_page + 1}' in href:\n                if href.startswith('/'):\n                    href = urljoin(current_url, href)\n                return href\n\n        return None\n","size_bytes":3317},"app/lance_scraper.py":{"content":"\"\"\"\nLance scraper compatible with the multi-source system\n\"\"\"\n\nimport logging\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom .base_scraper import BaseScraper\nfrom .utils import get_user_agent\n\nlogger = logging.getLogger(__name__)\n\n\nclass LanceScraper(BaseScraper):\n    \"\"\"Scraper specifically designed for LANCE! news site\"\"\"\n    \n    def get_site_domain(self):\n        \"\"\"Return the main domain for LANCE!\"\"\"\n        return \"lance.com.br\"\n    \n    def extract_article_links(self, html, base_url, section=None):\n        \"\"\"Extract article links from LANCE! listing page HTML\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        links = []\n        \n        # Find all links that point to articles (ending in .html)\n        for link in soup.find_all('a', href=True):\n            href = link.get('href', '')\n            \n            # Convert relative URLs to absolute\n            if href.startswith('/'):\n                href = urljoin(base_url, href)\n            elif not href.startswith(('http://', 'https://')):\n                continue\n                \n            # Only include LANCE! articles\n            if 'lance.com.br' not in href:\n                continue\n                \n            # Filter for article URLs (typically end with .htm or have date patterns)\n            if (href.endswith('.htm') or \n                href.endswith('.html') or \n                any(char.isdigit() for char in href.split('/')[-1:])):  # URL has date/ID\n                links.append(href)\n        \n        # Remove duplicates while preserving order\n        unique_links = []\n        seen = set()\n        for link in links:\n            if link not in seen:\n                unique_links.append(link)\n                seen.add(link)\n        \n        logger.info(f\"Found {len(unique_links)} article links on LANCE! page\")\n        return unique_links\n    \n    def find_next_page_url(self, html, current_url):\n        \"\"\"Find the URL for the next page of articles\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        \n        # Look for pagination links (common patterns)\n        next_selectors = [\n            'a[rel=\"next\"]',\n            'a.next',\n            'a.pagination-next',\n            'a[aria-label*=\"next\"]',\n            'a[aria-label*=\"Next\"]',\n            'a[aria-label*=\"pr√≥xima\"]',\n            'a[aria-label*=\"Pr√≥xima\"]'\n        ]\n        \n        for selector in next_selectors:\n            next_link = soup.select_one(selector)\n            if next_link and next_link.get('href'):\n                next_href = next_link.get('href')\n                if next_href.startswith('/'):\n                    return urljoin(current_url, next_href)\n                elif next_href.startswith(('http://', 'https://')):\n                    return next_href\n        \n        return None\n    \n    def parse_article_metadata(self, html, url):\n        \"\"\"Parse article metadata from LANCE! article page\"\"\"\n        soup = BeautifulSoup(html, 'lxml')\n        \n        # Try JSON-LD first (most reliable)\n        json_ld_data = self.parse_json_ld(html)\n        if json_ld_data:\n            return json_ld_data\n        \n        # Fallback to meta tags\n        return self._parse_meta_tags(soup, url)\n    \n    def _parse_meta_tags(self, soup, url):\n        \"\"\"Parse article metadata from meta tags\"\"\"\n        def get_meta_content(property_name):\n            tag = soup.find('meta', {'property': property_name}) or soup.find('meta', {'name': property_name})\n            return tag.get('content', '').strip() if tag else ''\n        \n        # Extract basic metadata\n        title = (get_meta_content('og:title') or \n                get_meta_content('twitter:title') or \n                soup.find('title').get_text().strip() if soup.find('title') else '')\n        \n        description = (get_meta_content('og:description') or \n                      get_meta_content('twitter:description') or \n                      get_meta_content('description') or '')\n        \n        image = (get_meta_content('og:image') or \n                get_meta_content('twitter:image') or '')\n        \n        # Clean up image URL\n        if image:\n            image = self._clean_image_url(image)\n        \n        # Extract author\n        author = (get_meta_content('article:author') or \n                 get_meta_content('author') or '')\n        \n        # Extract publish date\n        pub_date_str = (get_meta_content('article:published_time') or \n                       get_meta_content('article:published') or\n                       get_meta_content('datePublished') or '')\n        \n        pub_date = None\n        if pub_date_str:\n            try:\n                from dateutil import parser\n                pub_date = parser.parse(pub_date_str)\n            except Exception as e:\n                logger.warning(f\"Could not parse date '{pub_date_str}': {e}\")\n        \n        return {\n            'title': title,\n            'description': description,\n            'image': image,\n            'author': author,\n            'pub_date': pub_date,\n            'url': url\n        }\n    \n    def _clean_image_url(self, image_url):\n        \"\"\"Clean and validate image URL\"\"\"\n        if not image_url:\n            return None\n            \n        # Remove query parameters that might cause issues\n        if '?' in image_url:\n            image_url = image_url.split('?')[0]\n        \n        # Ensure absolute URL\n        if image_url.startswith('/'):\n            image_url = urljoin('https://www.lance.com.br', image_url)\n        \n        # Validate URL format\n        try:\n            parsed = urlparse(image_url)\n            if parsed.scheme in ['http', 'https'] and parsed.netloc:\n                return image_url\n        except:\n            pass\n        \n        return None","size_bytes":5854}},"version":1}